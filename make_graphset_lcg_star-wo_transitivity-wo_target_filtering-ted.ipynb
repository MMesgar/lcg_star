{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import general packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_path = './glove.840B.300d.txt'\n",
    "\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(seed=1)\n",
    "\n",
    "cn = 0\n",
    "word2vec = {}\n",
    "with open(w2v_path,'rb') as w2v:\n",
    "    content = w2v.read().strip()\n",
    "    for line in content.split('\\n'):\n",
    "        cn +=1\n",
    "        line = line.strip().split()\n",
    "        v = line[0]\n",
    "        \n",
    "        vector = line[1:]\n",
    "        vector = np.matrix(vector,dtype='float32')\n",
    "        \n",
    "        word2vec[v] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def apply_rule4(graph):\n",
    "    #Rule 4\n",
    "    adj_list= deepcopy(graph)\n",
    "    for sent_id, edges in enumerate(adj_list):\n",
    "        for edge in edges:\n",
    "            if edge[-1] == 'trans':\n",
    "                continue\n",
    "            target_sent_id = edge[0]\n",
    "            source_word = edge[1]\n",
    "            target_word = edge[2]\n",
    "            if target_sent_id != -1:\n",
    "                trans_source_id = sent_id\n",
    "                trans_source_word = source_word\n",
    "                trans_weight = 'trans'\n",
    "               \n",
    "                j_edges = adj_list[target_sent_id]\n",
    "                tmp = [e for e in j_edges if e[1]==target_word]\n",
    "                for item in tmp:\n",
    "                    trans_target_id = item[0]\n",
    "                    trans_target_word = item[2]\n",
    "                    trans_edge = (trans_target_id, trans_source_word, trans_target_word, trans_weight)\n",
    "                    adj_list[sent_id].append(trans_edge)\n",
    "\n",
    "    return adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from scipy import spatial\n",
    "def create_graph(list_of_sents):\n",
    "    \n",
    "    # get the number of sentences or nodes\n",
    "    n = len(list_of_sents)\n",
    "    \n",
    "    #initialize the adjacent list for the graph representation of this text\n",
    "    adj_list = [[(-1,None,None, 0.0)]]*n\n",
    "    max_weight = [0.0]*n\n",
    "\n",
    "    #for each sentence check the similarity of each word with all previous words of all connections\n",
    "    word_connections = dict()\n",
    "    \n",
    "    for i in range(1,n):            \n",
    "            sent_i = list_of_sents[i]\n",
    "                                 \n",
    "            for w in sent_i:\n",
    "                \n",
    "                word_connections[(i,w)] = []#(-1.0, None, -1.0)\n",
    "                max_weight_word_connection = 0.0\n",
    "                try:\n",
    "                    w_vec = word2vec[w]\n",
    "                except:\n",
    "                    word2vec[w] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "                    w_vec = word2vec[w]\n",
    "                \n",
    "                #w_vec = get_word_vector(w)    \n",
    "                for j, sent_j in enumerate(list_of_sents[:i]):\n",
    "                    for v in sent_j:\n",
    "                        try:\n",
    "                            v_vec = word2vec[v]\n",
    "                        except:\n",
    "                            word2vec[v] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "                            v_vec = word2vec[v]\n",
    "                            \n",
    "                        #v_vec = get_word_vector(v)\n",
    "                        cosine_w_v =  np.abs(1 - spatial.distance.cosine(w_vec, v_vec))\n",
    "                        cosine_w_v = np.round(cosine_w_v,4)\n",
    "                        if cosine_w_v > max_weight_word_connection: #Rule 1: connect to the nearest sentence\n",
    "                            word_connections[(i,w)] = [(j, v, cosine_w_v)]\n",
    "                            max_weight_word_connection = cosine_w_v\n",
    "                        elif cosine_w_v == max_weight_word_connection:\n",
    "                            word_connections[(i,w)].append((j, v, cosine_w_v))\n",
    "                # Rule 2\n",
    "                if max_weight_word_connection > max_weight[i]:\n",
    "                    adj_list[i] = [(item[0],\n",
    "                                    w,\n",
    "                                    item[1],\n",
    "                                    item[2])\n",
    "                                  for item in word_connections[(i,w)]\n",
    "                                  ]\n",
    "                    max_weight[i] = max_weight_word_connection\n",
    "                    \n",
    "                # Rule 3\n",
    "                elif max_weight_word_connection == max_weight[i]:\n",
    "                    flag = False # do both connect sent_i to the same sentence \n",
    "                    for item in adj_list[i]:\n",
    "                        if item[0] in [wc[0] for wc in word_connections[(i,w)]]:\n",
    "                            flag = True\n",
    "                            break\n",
    "                    if flag == False:\n",
    "                        adj_list[i] +=  [(item[0], w, item[1], item[2])\n",
    "                         for item in word_connections[(i,w)]]\n",
    "    #Rule 4\n",
    "    #adj_list = apply_rule4(adj_list)\n",
    "    return adj_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(graph_set, path):\n",
    "    output_content = []\n",
    "    for graph_name,  adj_list in enumerate(graph_set):\n",
    "        output_content.append('XP')\n",
    "        output_content.append('% '+str(graph_name))\n",
    "        #output_content.append('t # %d'%graph_name)\n",
    "        num_nodes = len(adj_list)\n",
    "\n",
    "        for n in range(num_nodes):\n",
    "            output_content.append('v %s a'%str(n))\n",
    "\n",
    "        for i, edges in enumerate(adj_list):\n",
    "            if i>0:\n",
    "                for j in edges:\n",
    "                    #Note: we computed edges backward, but we should \n",
    "                    # save them forward to be compatible with NAACL16\n",
    "                    source = j[0] \n",
    "                    target = i\n",
    "                    if target > source:\n",
    "                        if source>0 and target>0:\n",
    "                            output_content.append('d %s %s 1'%(str(source),str(target)))\n",
    "                    else:\n",
    "                        raise ValueError(\"Backward eadge?\")\n",
    "\n",
    "    with open(path,'wb') as out:\n",
    "        out.write('\\n'.join(output_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def drawProgressBar(shell_out, \n",
    "                    begin, k, out_of, end, barLen =25):\n",
    "    percent = k/float(out_of)\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        elif i==int(barLen * percent):\n",
    "            progress +='>'\n",
    "        else:\n",
    "            progress += \"_\"\n",
    "    text = \"%s%d/%d[%s](%.2f%%)%s\"%(begin,k,out_of,progress,percent * 100, end)\n",
    "    if shell_out== True:\n",
    "        sys.stdout.write(text)\n",
    "        sys.stdout.flush()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./ted-gender-annotated/dataset.pkl\",'rb') as f:\n",
    "    talks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of male texts:1012\n",
      "number of female texts:344\n"
     ]
    }
   ],
   "source": [
    "male_texts = []\n",
    "female_texts = []\n",
    "\n",
    "for talk_id, talk in talks.items():\n",
    "    if talk['gender'] == 'male':\n",
    "        male_texts.append(talk['content'])\n",
    "    else:\n",
    "        female_texts.append(talk['content'])\n",
    "print \"number of male texts:%d\"%len(male_texts)\n",
    "print \"number of female texts:%d\"%len(female_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1012 out of 1012 | elapsed: 44.8min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "male_graphs = []\n",
    "male_graphs = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(create_graph), male_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(male_graphs,'./ted-gender-annotated/male_graphset.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 344 | elapsed: 22.1min finished\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "female_graphs = []\n",
    "female_graphs = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(create_graph), female_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(female_graphs,'./ted-gender-annotated/female_graphset.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump_text = \"Barak is the US president . Angela is the chancellor of Germany . Barak and Angela met each other in France .\"\n",
    "#dump_text = \"Mohsen likes briliant people . Ali tries hard . Mohsen is a successful mohsen person . Mohsen is smart . \"\n",
    "#dump_text = pre_process_par(dump_text)\n",
    "#print dump_text\n",
    "#adj_list = create_graph(dump_text)\n",
    "#print word_connections\n",
    "#g = adj_list\n",
    "#for k, v in enumerate(g):\n",
    "#    print \"%d -->  %s\"%(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('original_pars_graphs.pickle','rb') as out:\n",
    "#    graphs  = pickle.load(out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data first. We have two corpora: \n",
    "         - original\n",
    "         - translated\n",
    "Each dataset consists of some paragraphs. Paragraphs are seperated by an empty line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of paragraphs. We need to pre-process each paragraph and remove all stop words from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should process and clean all pragraphs in both corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a paragraph and build its graph. In this regard we define a function that takes a list of sentenecs (tokenized) as an input text and returns a graph (in ?????? format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_text = \" Barak is the US president . Angela is the chancellor of Germany . Barak and Angela met each other in France .\"\n",
    "#dump_text = \"Mohsen likes Ali people . Mohsen tries Ali hard . Mohsen is a successful person . Mohsen is smart . \"\n",
    "\n",
    "dump_text = pre_process_par(dump_text)\n",
    "print dump_text\n",
    "adj_list = create_graph(dump_text)\n",
    "\n",
    "g = adj_list\n",
    "for k, v in enumerate(g):\n",
    "    print \"%d -->  %s\"%(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_connections, adj_list = graphs[0]\n",
    "#for k,v in enumerate(adj_list):\n",
    "#    if len(v)>1:\n",
    "#        print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = graphs[0][1]\n",
    "#for k, v in enumerate(g):\n",
    "#    print \"%d --> %d %s\"%(k,v[0],v[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct graphs for translationese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
