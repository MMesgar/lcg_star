{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                  author: Mohsen Mesgar\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import sys, copy, time, math, pickle\n",
    "import cPickle as cpickle\n",
    "import itertools\n",
    "import scipy.io\n",
    "import pynauty\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys, getopt\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from subgraph_struc import read, write,get_canonical_map,draw, graph_to_adj_matrix as graph2am, recover_graph, draw\n",
    "from graph_set import read_graph_set as read_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def drawProgressBar(shell_out, \n",
    "                    begin, k, out_of, end, barLen =25):\n",
    "    percent = k/float(out_of)\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        elif i==int(barLen * percent):\n",
    "            progress +='>'\n",
    "        else:\n",
    "            progress += \"_\"\n",
    "    text = \"%s%d/%d[%s](%.2f%%)%s\"%(begin,k,out_of,progress,percent * 100, end)\n",
    "    if shell_out== True:\n",
    "        sys.stdout.write(text)\n",
    "        sys.stdout.flush()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                   get pattern canonical map without node order\n",
    "###############################################################################\n",
    "def get_canonical_map(g):\n",
    "    if len(g.nodes())>0:\n",
    "        a = nx.adjacency_matrix(g)\n",
    "        am = a.todense()\n",
    "        window = np.array(am)\n",
    "        adj_mat = {idx: [i for i in list(np.where(edge)[0]) if i!=idx] for idx, edge in enumerate(window)}\n",
    "#       This line doesn't take into account the order of nodes, it produces the identical\n",
    "#       canonoical map for these graphs\n",
    "#       0-->1 2, 0 1-->2, 0-->2 1\n",
    "#        tmp = pynauty.Graph(number_of_vertices=len(g.nodes()), directed=True, adjacency_dict = adj_mat) \n",
    "\n",
    "        tmp = pynauty.Graph(number_of_vertices=len(g.nodes()), directed=True, adjacency_dict = adj_mat, \n",
    "                    vertex_coloring = [set([t]) for t in range(len(g.nodes(0)))],) \n",
    "\n",
    "        cert = pynauty.certificate(tmp)\n",
    "    else:\n",
    "        cert = ''\n",
    "    return cert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                               read graph maps\n",
    "###############################################################################\n",
    "def get_maps(can_map_file, count_file):\n",
    "    # canonical_map -> {canonical string id: {\"graph\", \"idx\", \"n\"}}\n",
    "    canonical_map = read(can_map_file)\n",
    "    \n",
    "   \n",
    "   # weight map -> {parent id: {child1: weight1, ...}}\n",
    "    weight_map = read(count_file)\n",
    "    \n",
    "    \n",
    "    weight_map = {parent: {child: weight/float(sum(children.values())) for child, weight in children.items()} \n",
    "                    for parent, children in weight_map.items()}\n",
    "    child_map = {}\n",
    "    for parent, children in weight_map.items():\n",
    "        for k,v in children.items():\n",
    "            if k not in child_map:\n",
    "                child_map[k] = {}\n",
    "            child_map[k][parent] = v\n",
    "    weight_map = child_map\n",
    "    return canonical_map, weight_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                  compute the base probability\n",
    "###############################################################################\n",
    "def pb(graph_id, weight_map):\n",
    "    parents =  weight_map[graph_id] \n",
    "    total = 0    \n",
    "    for k,w in parents.items():\n",
    "        total = w*pb(k, weight_map)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "###############################################################################\n",
    "##                compute the count of each pattrn in each graph\n",
    "###############################################################################\n",
    "def pattern_counter_in_graph(inputs):\n",
    "    gidx = inputs[0]\n",
    "    graph = inputs[1]\n",
    "    min_pattern_size = inputs[2] \n",
    "    max_pattern_size = inputs[3]\n",
    "    samplesize = inputs[4] \n",
    "    canonical_map = inputs[5]\n",
    "    \n",
    "    # in case we don't observe any graphlet in the graph, we fallback to the graphlet id that has zero edges in it\n",
    "    #fallback_map = {1: 1, 2: 2, 3: 4, 4: 8, 5: 19, 6: 53,       7: 209, 8: 1253, 9: 13599}\n",
    "    fallback_map = {1: 1, 2: 3, 3: 11, 4: 75, 5: 1099, 6: 13901}\n",
    "    # initialize the seed \t\n",
    "    seed = 1        \n",
    "    np.random.seed(seed)   \n",
    "    \n",
    "    am = graph2am(graph)\n",
    "    graph_size = len(am)\n",
    "    \n",
    "    # count_map = {node id: absolute count, ...}\n",
    "    count_map = {}\n",
    "    \n",
    "    \n",
    "    for pattern_size in range(min_pattern_size, max_pattern_size+1):\n",
    "        #print \"pattern_size=\", pattern_size\n",
    "        \n",
    "        # we don't need to loop if size of the adj. matrix is smaller than n        \n",
    "        if graph_size >= pattern_size:\n",
    "            count = 0\n",
    "            sample_set =[]\n",
    "            ub = scipy.misc.comb(graph_size, pattern_size)\n",
    "            while (len(sample_set) <= samplesize and len(sample_set) < ub):\n",
    "                #print \"sample_set=\", sample_set\n",
    "                r = random.sample(range(graph_size), pattern_size)\n",
    "                r_sort = np.sort(r).tolist()\n",
    "                \n",
    "                #print \"r\",r\n",
    "                #print \"r_sort\",r_sort\n",
    "                \n",
    "                if sample_set.count(r_sort)==0:\n",
    "                    sample_set.append(r_sort)\n",
    "                    count = count + 1\n",
    "                #print \"count\",count\n",
    "                \n",
    "        #    for s in range(samplesize):\n",
    "            #print \"final_sample_set=\", sample_set\n",
    "            #print \"final_count\", count\n",
    "            for s in sample_set:\n",
    "                #print \"sample=\",s\n",
    "                window = am[np.ix_(s,s)]\n",
    " \n",
    "                # fekr konam window bayyad ye jori graph bashe\n",
    "                pattern = nx.DiGraph(window)\n",
    "                g_type = canonical_map[get_canonical_map(pattern)][\"idx\"]               \n",
    "                #print \"g_type\", g_type\n",
    "                \n",
    "                # increment the count of seen graphlet\n",
    "                count_map[g_type] = count_map.get(g_type,0)+ 1.0\n",
    "                #print  \"count_map[g_type]\", count_map[g_type]\n",
    "\n",
    "        else:\n",
    "            # fallback to 0th node at that level\n",
    "            count_map[fallback_map[pattern_size]] = samplesize\n",
    "            #print \"In_fall_back\",\"count_map[fallback_map[pattern_size]]\",count_map[fallback_map[pattern_size]]\n",
    "  \n",
    "    return (gidx, count_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##           compute the count of subgraphs for each graph in graph set\n",
    "## graph_set: a dictionary of graphs and their id. {idx1:graph1, idx2:graph2,...}\n",
    "###############################################################################\n",
    "from joblib import Parallel, delayed\n",
    "def count_subgraphs(graph_set_file, min_pattern_size, max_pattern_size, sample_size, can_map, output_file):\n",
    " \n",
    "    ## read graph_set\n",
    "    print \"loading the graph_set_file ...: %s\"%graph_set_file\n",
    "    graph_set = read_graph_set(graph_set_file)\n",
    "    print \"# graphs in graph_set: %d\"%len(graph_set)\n",
    "\n",
    "    #    for gidx, value in graph_set.items():\n",
    "#        graph = value['graph']\n",
    "        #print \"graph_name =\"+ value['name'] +\" id=\" + str(gidx)\n",
    "#        graph_map[gidx] = sample_worker(graph,min_pattern_size,max_pattern_size, sample_size, can_map)\n",
    "        #print 'graph_'+str(gidx)+' is processed'\n",
    "    \n",
    "    print \"start counting patterns ...\"\n",
    "    input_graphs = [(gidx, value['graph'],min_pattern_size,max_pattern_size, sample_size, can_map) for gidx, value in graph_set.items()]  \n",
    "    \n",
    "    graph_map = []\n",
    "    for i,graph in enumerate(input_graphs):\n",
    "        graph_map.append(pattern_counter_in_graph(graph))\n",
    "        drawProgressBar(shell_out=True, \n",
    "                    begin=\"\", \n",
    "                        k=i+1, out_of=len(input_graphs), \n",
    "                        end=\"\")\n",
    "#     graph_map = Parallel(n_jobs=2, verbose=1, backend=\"multiprocessing\")(\n",
    "#        map(delayed(pattern_counter_in_graph), input_graphs))\n",
    "\n",
    "    ## which patterns occures how often\n",
    "    graph_map = { x:y for (x,y) in graph_map}   \n",
    "    \n",
    "    write(graph_map, output_file)\n",
    "    print \"\\ngraph_map is saved here : %s\"%output_file\n",
    "    return graph_map\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "###############################################################################\n",
    "##                               read graph set\n",
    "###############################################################################\n",
    "def read_graph_set(graph_set_file):\n",
    "    return read_gs(graph_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                      find the ids of all patterns with ps nodes\n",
    "## ps: pattern size\n",
    "###############################################################################\n",
    "def k_node_graphs(can_map, ps):\n",
    "    output = {v['idx']  for k,v in can_map.items() if v['n']==ps } # connected + disconnected\n",
    "#     output = {v['idx']  for k,v in can_map.items() if v['n']== ps and \n",
    "#               nx.is_weakly_connected(get_subgraph(v['idx'], can_map))} # only connected\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##         compute the sum over all count of k-node subgraphs\n",
    "## can_map = {graph_canonical_map:{'graph':..., 'idx':,..., 'n':....}}\n",
    "## k: k-node subgraphs, it shows the depth of the tree himap\n",
    "###############################################################################\n",
    "def z(all_knode_patterns, pat_cnt):\n",
    "    filter_pattern_count = {k:v for k,v in pat_cnt.items() if (k in all_knode_patterns)}\n",
    "    return float(0.1+sum([v for v in filter_pattern_count.values()]))\n",
    "    #return float(sum([v for v in filter_pattern_count.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                 n_c: number of paterns with exatly count e\n",
    "###############################################################################\n",
    "def n(e, pat_cnt):\n",
    "    l= pat_cnt.values()\n",
    "    return float(l.count(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                       compute discount value d\n",
    "## n_c:number of patterns with exactly count c\n",
    "###############################################################################\n",
    "def disc(c, pat_cnt):\n",
    "    n1 = n(1, pat_cnt)\n",
    "    n2 = n(2, pat_cnt)\n",
    "    n3 = n(3, pat_cnt)\n",
    "    n4 = n(4, pat_cnt)\n",
    "    y = float(n1) / n1+2*n2\n",
    "    if c==0:\n",
    "        return 0\n",
    "    elif c==1:\n",
    "        return y\n",
    "    elif c==2:\n",
    "        return 2-3*y*(float(n3)/n2)\n",
    "    else:\n",
    "        return 2-3*y*(float(n4)/n3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##               compute probability based on the frequency\n",
    "###############################################################################\n",
    "def pf(pattern_idx,pattern_count, d, z_value):\n",
    "    if (pattern_idx in pattern_count.keys()):\n",
    "        count = pattern_count[pattern_idx]\n",
    "    else:\n",
    "        count = 0\n",
    "   # d = disc(count, pattern_count)\n",
    "    nominator = max(count-d,0)\n",
    "    \n",
    "    denominator = z_value\n",
    "    prob =  float(nominator)/float(denominator)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                 Normalization factor for base probability\n",
    "###############################################################################\n",
    "def norm_fact(all_knode_patterns, pattern_count, d):\n",
    "    filter_pattern_count = {k:v for k,v in pattern_count.items() if (k in all_knode_patterns)}\n",
    "    num_nn = len([v for v in filter_pattern_count.values() if v >= d])\n",
    "    b= sum([v for v in filter_pattern_count.values() if v < d])\n",
    "    return num_nn, b\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "###############################################################################\n",
    "##                               Mass value\n",
    "###############################################################################\n",
    "def mass(d, z_value, norm_fact, bounes):    \n",
    "    return (d/z_value)*norm_fact +(bounes/z_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                       compute base probability of a pattern\n",
    "## pb('')=pb(1)=1 because those occur in every possible graph\n",
    "###############################################################################\n",
    "def pb(wm, parent_kn,  pattern_id):\n",
    "    prob_base = 0\n",
    "    if pattern_id ==0 :\n",
    "        prob_base=1\n",
    "    else:\n",
    "        for parent_id, weight in wm[pattern_id].items():\n",
    "            prob_base = prob_base + pb(wm,parent_kn, parent_id)*weight\n",
    "            #prob_base += (parent_kn[parent_id]*weight)\n",
    "    return prob_base  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##           KN probability of the given pattern in the given graph\n",
    "## pattern count == pc[graph_id]\n",
    "## ps is pattern_size= number of nodes\n",
    "###############################################################################\n",
    "def pkn(can_map, pattern_count, w_map,parent_kn, pattern_idx, pattern_size, d, z_value, all_knode_patterns):\n",
    "   # all_knode_patterns = k_node_graphs(can_map, pattern_size)\n",
    "   # z_value = z(all_knode_patterns, pattern_count)\n",
    "    p1= pf(pattern_idx, pattern_count, d, z_value)\n",
    "    if (d==0):\n",
    "        pkn = p1 \n",
    "    else:\n",
    "        p2 = pb(w_map,parent_kn, pattern_idx)\n",
    "        mass_factor , bonus = norm_fact(all_knode_patterns, pattern_count, d)\n",
    "        mass_value = mass(d, z_value,mass_factor, bonus)\n",
    "        pkn = p1 + (mass_value*p2)\n",
    "        \n",
    "    parent_kn[pattern_idx] = pkn\n",
    "    return pkn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                          compute graph vector\n",
    "## pc : pattern count in each graph of graph_set\n",
    "###############################################################################\n",
    "def get_graph_vector(pc, can_map, wei_map,parent_kn, number_nodes, d):\n",
    "    graph_vectores = {}\n",
    "    all_knode_patterns = k_node_graphs(can_map, number_nodes)\n",
    "    #print all_knode_patterns\n",
    "    for graph_id, patt_cnt in pc.items():\n",
    "        tmp_vect = {}\n",
    "        #print pc[graph_id]\n",
    "        #print all_knode_patterns\n",
    "        z_value = z(all_knode_patterns, pc[graph_id])\n",
    "        #print \"graph_id=\"+str(graph_id) + \" z_value=\" + str(z_value)\n",
    "        for pid in k_node_graphs(can_map, number_nodes):\n",
    "            p_pkn = pkn(can_map, pc[graph_id], wei_map,parent_kn, pid, number_nodes, d,z_value,all_knode_patterns)\n",
    "            tmp_vect[pid]=p_pkn\n",
    "        graph_vectores[graph_id] = tmp_vect\n",
    "    return graph_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                      find a graph in the can_map\n",
    "###############################################################################\n",
    "def get_subgraph(gidx, can_map):\n",
    "    tmp = [t for t in can_map.values() if t['idx']==int(gidx)][0]\n",
    "    graph= tmp['graph']\n",
    "    n= tmp['n']\n",
    "    g = recover_graph(graph,n, gidx)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                 data points for classification\n",
    "###############################################################################    \n",
    "def data_points(hs, m):\n",
    "    instances = []\n",
    "    count = 0\n",
    "    for i in range(1,len(hs)+1):\n",
    "        for j in range(i+1, len(hs)+1):\n",
    "            label = -1 #'B'\n",
    "            d = hs[i]-hs[j]\n",
    "            if (math.fabs(d)>0.5):\n",
    "                if d>0:\n",
    "                    label = +1#'A'\n",
    "                count = count + 1\n",
    "                inst =m[i-1,:].tolist()[0] + m[j-1,:].tolist()[0]+[label]\n",
    "                instances.append(inst) \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count_of_connected_patterns_of_a_graph(pc_graph, can_map):\n",
    "    output = []\n",
    "    for idx in pc_graph.keys():\n",
    "        g = get_subgraph(idx,can_map)\n",
    "        #if nx.is_weakly_connected(g):\n",
    "            #print \"idx: %d\"%idx\n",
    "            #print \"nodes : %s\"%g.nodes()\n",
    "            #print \"edges : %s\"%g.edges()\n",
    "            #print \"count : %d\"%pc_graph[idx]\n",
    "            #print \"------\"\n",
    "        output.append((idx,pc_graph[idx]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class count_matrix(object):\n",
    "    def __init__(self, name, pattern_ids, graph_ids,count_matrix):\n",
    "        self.pattern_ids = pattern_ids\n",
    "        self.graph_ids = graph_ids\n",
    "        self.count_matrix = count_matrix\n",
    "        self.name = name\n",
    "    \n",
    "    def display_patterns(self, can_map):\n",
    "        for idx in self.pattern_ids:\n",
    "            g = get_subgraph(idx,can_map)\n",
    "            print \"idx: %d\"%idx\n",
    "            print \"nodes : %s\"%g.nodes()\n",
    "            print \"edges : %s\"%g.edges()\n",
    "            print \"------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##  Here we go step by step over our experiments on translationese and original\n",
    "###############################################################################\n",
    "def ted_experiment(num_nodes):\n",
    "    min_pattern_size = num_nodes\n",
    "    max_pattern_size = num_nodes\n",
    "    sample_size = 2000 # numbrt of samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    normalized = True\n",
    "    print \"min_pattern_size: %d\"%min_pattern_size\n",
    "    print \"max_pattern_size: %d\"%max_pattern_size\n",
    "    print \"sample_size: %d\"%sample_size\n",
    "    \n",
    "    \n",
    "    can_map_file = \"./canonical_map/can_map_maxk6.p\"\n",
    "    himap_file = \"./canonical_map/himap_maxk6.p\"\n",
    "\n",
    "    \n",
    "    subgraph_count_file = \"./count_orig_graph_set\"+\"_min:\"+ str(min_pattern_size)+\"_max:\"+str(max_pattern_size)\n",
    "    \n",
    "    print \"loading can_map and hi_map: %s %s\"%(can_map_file, himap_file)\n",
    "    can_map, weight_map = get_maps(can_map_file, himap_file)    \n",
    "       \n",
    "    output = []\n",
    "    for gs_id,graph_set_file in enumerate([\"./ted-gender-annotated/male_graphset.g\",\n",
    "                                           \"./ted-gender-annotated/female_graphset.g\"]):\n",
    "        print \"processing: %s \"%graph_set_file\n",
    "        \n",
    "        pc = count_subgraphs(graph_set_file,\n",
    "                             min_pattern_size, max_pattern_size,\n",
    "                            sample_size,\n",
    "                             can_map, \n",
    "                             subgraph_count_file)\n",
    "\n",
    "        print \"pattern counting is done.\"\n",
    "    \n",
    "        all_count_matrices = {}\n",
    "        print \"computing the count matrices ...\"\n",
    "        for num_nodes in range(min_pattern_size,max_pattern_size+1):\n",
    "            #print \"pattern_size: %d\"%num_nodes\n",
    "            connected_patterns_idx = list(k_node_graphs(can_map,num_nodes))\n",
    "            #print \"list of all possible connected patterns (columns): %s\"%connected_patterns_idx\n",
    "            num_graphs = len(pc.keys())\n",
    "            num_patterns = len(connected_patterns_idx)\n",
    "            cnt_matrix = np.zeros((num_graphs,num_patterns))\n",
    "            #print \"graph ids in rows of count_matrix: %s\" %pc.keys()\n",
    "            for key in pc.keys():\n",
    "                count  = get_count_of_connected_patterns_of_a_graph(pc[key], can_map)\n",
    "                row = key\n",
    "                for (pattern_id, value) in count:\n",
    "                    if pattern_id not in connected_patterns_idx:\n",
    "                        continue\n",
    "                    col = connected_patterns_idx.index(pattern_id)\n",
    "                    cnt_matrix[row, col] = value\n",
    "            cm = count_matrix(num_nodes, connected_patterns_idx,pc.keys(),  cnt_matrix)\n",
    "            all_count_matrices[num_nodes] = cm \n",
    "        print \"all connected patterns are counted\"\n",
    "        output.append((graph_set_file, pc, can_map, all_count_matrices))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LCG count 3-node\n",
    "output = ted_experiment(num_nodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('./ted-gender-annotated/final_pattern_count_3_inc_disconnected_wo_trans_wo_distance.pkl','wb') as h:\n",
    "     cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 3-node count\n",
    "with open('./ted-gender-annotated/final_pattern_count_3_inc_disconnected_wo_trans_wo_distance.pkl','r') as h:\n",
    "    output = cpickle.load(h)\n",
    "\n",
    "pc_male, can_map_male, all_count_matrices_male = \\\n",
    "    output[0][1], output[0][2], output[0][3]\n",
    "\n",
    "pc_female, can_map_female, all_count_matrices_female = \\\n",
    "    output[1][1], output[1][2], output[1][3]\n",
    "    \n",
    "three_nodes_male = all_count_matrices_male[3]\n",
    "#three_nodes_original.display_patterns(can_map_original)\n",
    "\n",
    "three_nodes_female = all_count_matrices_female[3]\n",
    "\n",
    "x_male = three_nodes_male.count_matrix\n",
    "x_female = three_nodes_female.count_matrix\n",
    "\n",
    "print \"x_male.shape= %s\"%str(x_male.shape)\n",
    "print \"x_female.shape= %s\"%str(x_female.shape)\n",
    "\n",
    "x =  np.concatenate((x_male, x_female), axis=0)\n",
    "y = [1]*x_male.shape[0] + [0]*x_female.shape[0]\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)\n",
    "\n",
    "s = x.sum(1,keepdims=True)*1.0\n",
    "x = x/ s\n",
    "\n",
    "print \"x.shape: %s\"%str(x.shape)\n",
    "print \"y.len: %d\"%len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(class_weight=\"balanced\", random_state=random_state)\n",
    "#classifier = SVC(kernel='linear', class_weight='balanced',  probability=True)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "mean_m_acc= []\n",
    "mean_f_acc= []\n",
    "predictions_3node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    cm = confusion_matrix(y,predicted)\n",
    "    print cm\n",
    "    # compute accuracy of m:\n",
    "    f_acc = float(cm[0,0]) / (cm[0,0]+ cm[0,1])\n",
    "    m_acc  = float(cm[1,1]) / (cm[1,0]+cm[1,1])\n",
    "    mean_accs.append(acc)\n",
    "    mean_m_acc.append(m_acc)\n",
    "    mean_f_acc.append(f_acc)\n",
    "    print \"iteration:%d, acc:%.2f, m_acc:%.2f, f_acc: %.2f\"%(i,acc,m_acc,f_acc)\n",
    "    predictions_3node_LR+= list(predicted)\n",
    "print \"acc 3node 5-CV = %.2f%% , m_acc:%.2f, f_acc: %.2f \"%((100*np.mean(mean_accs)),100*np.mean(mean_m_acc),\n",
    "                                                            100*np.mean(mean_f_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-nodes: pattern frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load 3-node count\n",
    "with open('./ted-gender-annotated/final_pattern_count_3_inc_disconnected_wo_trans_wo_distance.pkl','r') as h:\n",
    "    output = cpickle.load(h)\n",
    "    \n",
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]\n",
    "\n",
    "three_nodes_original = all_count_matrices_original[3]\n",
    "three_nodes_original.display_patterns(can_map_original)\n",
    "\n",
    "print \"***********\\n\"\n",
    "three_nodes_trans = all_count_matrices_trans[3]\n",
    "three_nodes_trans.display_patterns(can_map_trans)\n",
    "\n",
    "x_original = three_nodes_original.count_matrix\n",
    "\n",
    "x_trans = three_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "l= 5\n",
    "\n",
    "xo = x_original\n",
    "s = xo.sum(1, keepdims=True)*1.0\n",
    "xo = xo / s\n",
    "yo = xo.sum(0,keepdims=True)\n",
    "yo =  yo.reshape(yo.shape[1])[k:k+l]\n",
    "\n",
    "xt = x_trans\n",
    "s = xt.sum(1, keepdims=True)*1.0\n",
    "xt = xt / s\n",
    "yt = xt.sum(0,keepdims=True)\n",
    "yt =  yt.reshape(yt.shape[1])[k:k+l]\n",
    "\n",
    "print yt[3] - yt[2] > 0 \n",
    "print yo-yt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = len(yo)\n",
    "x = np.arange(N)\n",
    "width = 0.35\n",
    "plt.bar(x, yo, width, color=\"blue\")\n",
    "plt.bar(x+width, yt, width,color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4-Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_pattern_size: 4\n",
      "max_pattern_size: 4\n",
      "sample_size: 2000\n",
      "loading can_map and hi_map: ./canonical_map/can_map_maxk6.p ./canonical_map/himap_maxk6.p\n",
      "processing: ./ted-gender-annotated/male_graphset.g \n",
      "loading the graph_set_file ...: ./ted-gender-annotated/male_graphset.g\n",
      "# graphs in graph_set: 1012\n",
      "start counting patterns ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mesgarmn/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `comb` is deprecated!\n",
      "Importing `comb` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.comb` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012[=========================](100.00%)\n",
      "graph_map is saved here : ./count_orig_graph_set_min:4_max:4\n",
      "pattern counting is done.\n",
      "computing the count matrices ...\n",
      "all connected patterns are counted\n",
      "processing: ./ted-gender-annotated/female_graphset.g \n",
      "loading the graph_set_file ...: ./ted-gender-annotated/female_graphset.g\n",
      "# graphs in graph_set: 344\n",
      "start counting patterns ...\n",
      "344/344[=========================](100.00%)\n",
      "graph_map is saved here : ./count_orig_graph_set_min:4_max:4\n",
      "pattern counting is done.\n",
      "computing the count matrices ...\n",
      "all connected patterns are counted\n"
     ]
    }
   ],
   "source": [
    "## comute 4-node count\n",
    "output = ted_experiment(num_nodes=4)\n",
    "with open('./ted-gender-annotated/final_pattern_count_4_inc_disconnected_wo_trans_wo_distance.pkl','wb') as h:\n",
    "     cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 4node count\n",
    "with open('./ted-gender-annotated/final_pattern_count_4_inc_disconnected_wo_trans_wo_distance.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_male.shape= (1012, 64)\n",
      "x_female.shape= (344, 64)\n",
      "x.shape: (1356, 64)\n",
      "y.len: 1356\n"
     ]
    }
   ],
   "source": [
    "pc_male, can_map_male, all_count_matrices_male = \\\n",
    "    output[0][1], output[0][2], output[0][3]\n",
    "\n",
    "pc_female, can_map_female, all_count_matrices_female = \\\n",
    "    output[1][1], output[1][2], output[1][3]\n",
    "    \n",
    "three_nodes_male = all_count_matrices_male[4]\n",
    "#three_nodes_original.display_patterns(can_map_original)\n",
    "\n",
    "three_nodes_female = all_count_matrices_female[4]\n",
    "\n",
    "x_male = three_nodes_male.count_matrix\n",
    "x_female = three_nodes_female.count_matrix\n",
    "\n",
    "print \"x_male.shape= %s\"%str(x_male.shape)\n",
    "print \"x_female.shape= %s\"%str(x_female.shape)\n",
    "\n",
    "x =  np.concatenate((x_male, x_female), axis=0)\n",
    "y = [1]*x_male.shape[0] + [0]*x_female.shape[0]\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)\n",
    "\n",
    "s = x.sum(1,keepdims=True)*1.0\n",
    "x = x/ s\n",
    "\n",
    "print \"x.shape: %s\"%str(x.shape)\n",
    "print \"y.len: %d\"%len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176 168]\n",
      " [442 570]]\n",
      "iteration:0, acc:0.55, m_acc:0.56, f_acc: 0.51\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions_3node_LR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-166156ac3a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmean_f_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"iteration:%d, acc:%.2f, m_acc:%.2f, f_acc: %.2f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mpredictions_3node_LR\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m print \"acc 4node 5-CV = %.2f%% , m_acc:%.2f, f_acc: %.2f \"%((100*np.mean(mean_accs)),100*np.mean(mean_m_acc),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_3node_LR' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(class_weight=\"balanced\", random_state=random_state)\n",
    "#classifier = SVC(kernel='linear', class_weight='balanced',  probability=True)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "mean_m_acc= []\n",
    "mean_f_acc= []\n",
    "predictions_4node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    cm = confusion_matrix(y,predicted)\n",
    "    print cm\n",
    "    # compute accuracy of m:\n",
    "    f_acc = float(cm[0,0]) / (cm[0,0]+ cm[0,1])\n",
    "    m_acc  = float(cm[1,1]) / (cm[1,0]+cm[1,1])\n",
    "    mean_accs.append(acc)\n",
    "    mean_m_acc.append(m_acc)\n",
    "    mean_f_acc.append(f_acc)\n",
    "    print \"iteration:%d, acc:%.2f, m_acc:%.2f, f_acc: %.2f\"%(i,acc,m_acc,f_acc)\n",
    "    predictions_3node_LR+= list(predicted)\n",
    "                                \n",
    "print \"acc 4node 5-CV = %.2f%% , m_acc:%.2f, f_acc: %.2f \"%((100*np.mean(mean_accs)),100*np.mean(mean_m_acc),\n",
    "                  100*np.mean(mean_f_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Pattern Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 4node count\n",
    "with open('./final_pattern_count_4_inc_disconnected_wo_trans_wo_distance.pkl','r') as h:\n",
    "    output = cpickle.load(h)\n",
    "    \n",
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]\n",
    "\n",
    "four_nodes_original = all_count_matrices_original[4]\n",
    "#three_nodes_original.display_patterns(can_map_original)\n",
    "\n",
    "four_nodes_trans = all_count_matrices_trans[4]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)\n",
    "\n",
    "#x_original = softmax(four_nodes_original.count_matrix)\n",
    "x_original = four_nodes_original.count_matrix\n",
    "\n",
    "#x_trans = softmax(four_nodes_trans.count_matrix)\n",
    "x_trans = four_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"number of 4-node patterns in orig: %d\"%x_original.shape[1]\n",
    "print \"number of 4-node patterns in trans: %d\"%x_trans.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 40\n",
    "\n",
    "xo = x_original\n",
    "s = xo.sum(1, keepdims=True)*1.0\n",
    "xo = xo / s\n",
    "yo = xo.sum(0,keepdims=True)\n",
    "yo =  yo.reshape(64)[k:k+10]\n",
    "\n",
    "xt = x_trans\n",
    "s = xt.sum(1, keepdims=True)*1.0\n",
    "xt = xt / s\n",
    "yt = xt.sum(0,keepdims=True)\n",
    "yt =  yt.reshape(64)[k:k+10]\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = len(yo)\n",
    "x = np.arange(N)\n",
    "width = 0.35\n",
    "plt.bar(x, yo, width, color=\"blue\")\n",
    "plt.bar(x+width, yt, width,color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x_trans\n",
    "s = x.sum(1, keepdims=True)*1.0\n",
    "x = x / s\n",
    "y = x.sum(0,keepdims=True)\n",
    "\n",
    "y=  y.reshape(64)[:10]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = len(y)\n",
    "x = range(N)\n",
    "width = 1/1.5\n",
    "plt.bar(x, y, width, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5-Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## comute 5-node count\n",
    "output = hansard_experiment(num_nodes=5)\n",
    "with open('./final_pattern_count_5_inc_disconnected_wo_trans_wo_distance.pkl','wb') as h:\n",
    "     cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 5node count\n",
    "with open('./final_pattern_count_5_inc_disconnected_wo_trans_wo_distance.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]\n",
    "\n",
    "five_nodes_original = all_count_matrices_original[5]\n",
    "#three_nodes_original.display_patterns(can_map_original)\n",
    "\n",
    "five_nodes_trans = all_count_matrices_trans[5]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)\n",
    "\n",
    "#x_original = softmax(four_nodes_original.count_matrix)\n",
    "x_original = five_nodes_original.count_matrix\n",
    "\n",
    "#x_trans = softmax(four_nodes_trans.count_matrix)\n",
    "x_trans = five_nodes_trans.count_matrix\n",
    "\n",
    "x =  np.concatenate((x_original, x_trans), axis=0)\n",
    "y = [1]*5000 + [0]*5000\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)\n",
    "\n",
    "s = x.sum(1,keepdims=True)*1.0\n",
    "x = x/ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_4node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_4node_LR += list(predicted)\n",
    "print \"acc 5node 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##  Here we go step by step over our experiments on translationese and original\n",
    "###############################################################################\n",
    "def hansard_baseline(num_nodes):\n",
    "    min_pattern_size = num_nodes\n",
    "    max_pattern_size = num_nodes\n",
    "    sample_size = 2000 # numbrt of samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    normalized = True\n",
    "    print \"min_pattern_size: %d\"%min_pattern_size\n",
    "    print \"max_pattern_size: %d\"%max_pattern_size\n",
    "    print \"sample_size: %d\"%sample_size\n",
    "    \n",
    "    \n",
    "    can_map_file = \"./canonical_map/can_map_maxk6.p\"\n",
    "    himap_file = \"./canonical_map/himap_maxk6.p\"\n",
    "\n",
    "    \n",
    "    subgraph_count_file = \"./count_orig_graph_set\"+\"_min:\"+ str(min_pattern_size)+\"_max:\"+str(max_pattern_size)\n",
    "    \n",
    "    print \"loading can_map and hi_map: %s %s\"%(can_map_file, himap_file)\n",
    "    can_map, weight_map = get_maps(can_map_file, himap_file)    \n",
    "       \n",
    "    output = []\n",
    "    for gs_id, graph_set_file in enumerate([\"./orig_graph_set_wo_trans_wo_distanc.g\",\"./trans_graph_set_wo_trans_wo_distanc.g\"]):\n",
    "        print \"processing: %s \"%graph_set_file\n",
    "        \n",
    "        pc = count_subgraphs(graph_set_file,\n",
    "                             min_pattern_size, max_pattern_size,\n",
    "                             sample_size,\n",
    "                             can_map, \n",
    "                             subgraph_count_file)\n",
    "        \n",
    "        print \"pattern counting is done.\"\n",
    "    \n",
    "        all_count_matrices = {}\n",
    "        print \"computing the count matrices ...\"\n",
    "        for num_nodes in range(min_pattern_size,max_pattern_size+1):\n",
    "            #print \"pattern_size: %d\"%num_nodes\n",
    "            connected_patterns_idx = list(k_node_graphs(can_map,num_nodes))\n",
    "            \n",
    "            #print \"list of all possible connected patterns (columns): %s\"%connected_patterns_idx\n",
    "            num_graphs = len(pc.keys())\n",
    "            num_patterns = len(connected_patterns_idx)\n",
    "            cnt_matrix = np.zeros((num_graphs,num_patterns))\n",
    "            #print \"graph ids in rows of count_matrix: %s\" %pc.keys()\n",
    "            for key in pc.keys():\n",
    "                count  = get_count_of_connected_patterns_of_a_graph(pc[key], can_map)\n",
    "                \n",
    "                row = key\n",
    "                for (pattern_id, value) in count:\n",
    "                    if pattern_id not in connected_patterns_idx:\n",
    "                        continue\n",
    "                    col = connected_patterns_idx.index(pattern_id)\n",
    "                    cnt_matrix[row, col] = value\n",
    "            cm = count_matrix(num_nodes, connected_patterns_idx,pc.keys(),  cnt_matrix)\n",
    "            all_count_matrices[num_nodes] = cm \n",
    "        print \"all connected patterns are counted\"\n",
    "        output.append((graph_set_file, pc, can_map, all_count_matrices))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "###############################################################################\n",
    "##                               Main\n",
    "###############################################################################\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "\n",
    "#output = hansard_experiment(num_nodes=3)\n",
    "#with open('./final_pattern_count_3.pkl','wb') as h:\n",
    "#    cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 3-node count\n",
    "with open('./final_pattern_count_3.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_nodes_original = all_count_matrices_original[3]\n",
    "#three_nodes_original.display_patterns(can_map_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_nodes_trans = all_count_matrices_trans[3]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x_original = softmax(three_nodes_original.count_matrix)\n",
    "x_original = three_nodes_original.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_trans = softmax(three_nodes_trans.count_matrix)\n",
    "x_trans = three_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =  np.concatenate((x_original, x_trans), axis=0)\n",
    "y = [1]*5000 + [0]*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s =  x.sum(1,keepdims=True)*1.0 \n",
    "x = x/ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_tsne = TSNE(learning_rate=100).fit_transform(x)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_3node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_3node_LR+= list(predicted)\n",
    "print \"acc 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random classification\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.dummy import DummyClassifier as baseline\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = baseline(strategy='most_frequent', random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_majority =[]\n",
    "\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    predictions_majority += list(predicted)\n",
    "    print i,acc\n",
    "print \"acc 5-Majority = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "import numpy as np\n",
    "def significant_test(predictions_1, predictions_2):\n",
    "    predictions_1 = np.array(predictions_1)\n",
    "    predictions_2 = np.array(predictions_2)\n",
    "    _,p_value = ttest(predictions_1,predictions_2)\n",
    "    if p_value < 0.01:\n",
    "        print \"significant with p_value < 0.01\"\n",
    "    elif p_value < 0.05:\n",
    "        print \"significant with p_value < 0.05\"\n",
    "    else:\n",
    "        print \"NOT significant\"\n",
    "    return p_value\n",
    "significant_test(predictions_majority,predictions_3node_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## comute 4-node count\n",
    "output = hansard_experiment(num_nodes=4)\n",
    "# with open('./final_pattern_count_4_4000.pkl','wb') as h:\n",
    "#     cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 4node count\n",
    "with open('./final_pattern_count_4_4000.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_nodes_original = all_count_matrices_original[4]\n",
    "#three_nodes_original.display_patterns(can_map_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_nodes_trans = all_count_matrices_trans[4]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_original = softmax(four_nodes_original.count_matrix)\n",
    "x_original = four_nodes_original.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_trans = softmax(four_nodes_trans.count_matrix)\n",
    "x_trans = four_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =  np.concatenate((x_original, x_trans), axis=0)\n",
    "y = [1]*5000 + [0]*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = x.sum(1,keepdims=True)*1.0\n",
    "x = x/ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_tsne = TSNE(learning_rate=100).fit_transform(x)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_4node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_4node_LR += list(predicted)\n",
    "print \"acc 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random classification\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.dummy import DummyClassifier as baseline\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = baseline(strategy='most_frequent', random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_majority =[]\n",
    "\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    predictions_majority += list(predicted)\n",
    "    print i,acc\n",
    "print \"acc 5-Majority = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "significant_test(predictions_3node_LR,predictions_4node_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
