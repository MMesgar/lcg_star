{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of talks: 1445\n",
      "number of gender annotations: 1415\n"
     ]
    }
   ],
   "source": [
    "with codecs.open('./ted-gender-annotated/ted-male-female-en.txt', 'rb',encoding=\"utf-8\") as f:\n",
    "    ted_male_female = f.read().strip()\n",
    "with open('./ted-gender-annotated/ted-gender-annotations-en.csv') as f:\n",
    "    ted_gender_annotation = f.read().strip()\n",
    "\n",
    "ted_male_female = ted_male_female.split('\\n\\n')\n",
    "ted_gender_annotation = ted_gender_annotation.split('\\n')\n",
    "\n",
    "print \"number of talks: %d\"%len(ted_male_female)\n",
    "print \"number of gender annotations: %d\"%len(ted_gender_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_talk(ted_male_female, url):\n",
    "    url = '<url>%s</url>'%url.strip()\n",
    "    for talk in ted_male_female:\n",
    "        talk_url = talk.split('\\n')[0]\n",
    "        if talk_url == url:\n",
    "            output =  talk.split('\\n')[1:]\n",
    "            return output\n",
    "    return None\n",
    "#get_talk(ted_male_female,'http://www.ted.com/talks/jessa_gamble_how_to_sleep.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# limit talk to ones that are annotated\n",
    "talks = dict()\n",
    "talk_id = 0\n",
    "count_discard = 0\n",
    "count_no_content = 0\n",
    "count_males = 0\n",
    "count_females = 0\n",
    "for gender_annotation in ted_gender_annotation:\n",
    "    gender = gender_annotation.split(',')[0].lower()\n",
    "    url = gender_annotation.split(',')[1]\n",
    "    \n",
    "    if gender in ['male','female']:\n",
    "        #identify the label\n",
    "        if gender == 'male':\n",
    "            label = 0\n",
    "            count_males += 1\n",
    "        else:\n",
    "            label =1 \n",
    "            count_females += 1 \n",
    "        # get the content\n",
    "        content = get_talk(ted_male_female, url)\n",
    "        if content != None:\n",
    "            talks[talk_id] = {'gender':gender,'label':label,'url':url.strip(),'content':content} \n",
    "            talk_id += 1\n",
    "        else:\n",
    "            count_no_content +=1 \n",
    "    else:\n",
    "        count_discard += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of discards: 59\n",
      "number of males: 1012\n",
      "number of females: 344\n",
      "number of talks in dataset: 1356\n"
     ]
    }
   ],
   "source": [
    "print \"number of discards: %d\"%count_discard\n",
    "print \"number of males: %d\"%count_males\n",
    "print \"number of females: %d\"%count_females\n",
    "print \"number of talks in dataset: %d\"% len(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of male texts:1012\n",
      "number of female texts:344\n"
     ]
    }
   ],
   "source": [
    "male_texts = []\n",
    "female_texts = []\n",
    "\n",
    "for talk_id,talk in talks.items():\n",
    "    if talk['gender'] == 'male':\n",
    "        male_texts.append(talk['content'])\n",
    "    else:\n",
    "        female_texts.append(talk['content'])\n",
    "print \"number of male texts:%d\"%len(male_texts)\n",
    "print \"number of female texts:%d\"%len(female_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# male_content = []\n",
    "# for text in male_texts:\n",
    "#     male_content.append('\\n'.join(text))\n",
    "\n",
    "# with codecs.open('./ted-gender-annotated/male.texts','wb','utf-8') as f:\n",
    "#     content = '\\n\\n'.join(male_content)\n",
    "#     f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# female_content = []\n",
    "# for text in female_texts:\n",
    "#     female_content.append('\\n'.join(text))\n",
    "\n",
    "# with codecs.open('./ted-gender-annotated/female.texts','wb','utf-8') as f:\n",
    "#     content = '\\n\\n'.join(female_content)\n",
    "#     f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n",
      "15328\n",
      "344\n"
     ]
    }
   ],
   "source": [
    "with codecs.open('./ted-gender-annotated/male.texts','rb','utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "count = 0\n",
    "for text in content.split('\\n\\n'):\n",
    "    if text!='\\n\\n':\n",
    "        count +=1\n",
    "print count\n",
    "        \n",
    "print len(content.split('\\n\\n')[0])\n",
    "\n",
    "with codecs.open('./ted-gender-annotated/female.texts','rb','utf-8') as f:\n",
    "    content = f.read()\n",
    "print len(content.split('\\n\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def voc_count(corpus):\n",
    "    d = defaultdict(int)\n",
    "    for p in corpus:\n",
    "        for sent in p:\n",
    "            for t in sent:\n",
    "                d[t] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#load list of stop words\n",
    "with open('./snowball_stopwords.txt','rb') as sw:\n",
    "    stop_words = [line.strip() for line in sw]\n",
    "\n",
    "#load punctuations \n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "#extra characters,\n",
    "extra = [u\"'s\", \n",
    "         u\"'m\", \n",
    "         u\"'re\",\n",
    "         u\"'ve\",\n",
    "         u\"'t\",\n",
    "         u\"'d\",\n",
    "         u\"'ll\",\n",
    "         u\",\",\n",
    "         u\"!\",\n",
    "         u\"(\",\n",
    "         u\")\",\n",
    "         u\"?\",\n",
    "         u'\"',\n",
    "         u'--',\n",
    "         u\"''\",\n",
    "         u'``'\n",
    "        ]\n",
    "\n",
    "def pre_process_par(par):\n",
    "    \"\"\"\n",
    "    input: \n",
    "       list of sentences\n",
    "    output:\n",
    "       list of sentences. Each sentence is a list of tokens.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    sent_par = par\n",
    "\n",
    "    # remove meta data\n",
    "    i = len(sent_par)-1\n",
    "    while(i>0):\n",
    "        sent = sent_par[i]\n",
    "        if (\"<talkid>\" in sent):\n",
    "            break\n",
    "        i = i - 1\n",
    "        \n",
    "    if i>0:\n",
    "        sent_par = sent_par[0:i]\n",
    "    \n",
    "     \n",
    "    # make the sentences lowecase\n",
    "    sent_par = [sent.lower() for sent in sent_par]\n",
    "    \n",
    "    \n",
    "      # tokenize and clean all sentences\n",
    "    for i,sent in enumerate(sent_par):\n",
    "        # remove extra things\n",
    "        for item in extra:\n",
    "            sent = sent.replace(item,'')\n",
    "        try:\n",
    "            #tokenize each sentence\n",
    "            tokens = word_tokenize(sent)\n",
    "        except ValueError:\n",
    "            print i\n",
    "            raise ValueError(sent)\n",
    "            return\n",
    "        # remove repetitve words in a sentenece\n",
    "        tokens = list(set(tokens))\n",
    "        \n",
    "        #remove stop words and clean texts\n",
    "        tokens = [tok for tok in tokens if \n",
    "                                          (tok not in stop_words) and \n",
    "                                          (tok not in punctuations)]\n",
    "        if len(tokens)>0:\n",
    "            # put it in the output\n",
    "            output.append(tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_path = './glove.840B.300d.txt'\n",
    "\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(seed=1)\n",
    "\n",
    "cn = 0\n",
    "word2vec = {}\n",
    "with open(w2v_path,'rb') as w2v:\n",
    "    content = w2v.read().strip()\n",
    "    for line in content.split('\\n'):\n",
    "        cn +=1\n",
    "        line = line.strip().split()\n",
    "        v = line[0]\n",
    "        \n",
    "        vector = line[1:]\n",
    "        vector = np.matrix(vector,dtype='float32')\n",
    "        \n",
    "        word2vec[v] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap(ds, word2vec):\n",
    "    voc_in_word2vec = set(word2vec.keys())\n",
    "    num_voc_in_word2vec = len(voc_in_word2vec)\n",
    "    print \"num_voc_in_word2vec: %d\"%num_voc_in_word2vec\n",
    "\n",
    "    voc_in_ds = []\n",
    "    for par in ds:\n",
    "        for sent in par:\n",
    "            voc_in_ds += sent\n",
    "    \n",
    "    num_voc_in_ds = len(voc_in_ds)\n",
    "    print \"num_voc_in_ds: %d\"%num_voc_in_ds\n",
    "    \n",
    "    print \"remove duplicates ... \"\n",
    "    voc_in_ds = set(voc_in_ds)\n",
    "    \n",
    "    num_voc_in_ds = len(voc_in_ds)\n",
    "    print \"num_voc_in_ds: %d\"%num_voc_in_ds\n",
    "    \n",
    "    intersection = set(voc_in_word2vec).intersection(voc_in_ds)\n",
    "    count_overlap = len(intersection)\n",
    "    print \"count_overlap: %d\"%count_overlap\n",
    "    \n",
    "    precent_overlap = 100*(count_overlap / float(num_voc_in_ds))\n",
    "    print \"precent_overlap: %.2f%%\"%precent_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statistics_ds(ds):\n",
    "    print \"data size (# talks): %d\"%len(ds)\n",
    "    \n",
    "    par_lens = [len(par) for par in ds]\n",
    "    avg_par_len = np.average(par_lens)\n",
    "    print \"avg_talk_len: %.2f\"%avg_par_len\n",
    "    \n",
    "    min_par_len = np.min(par_lens)\n",
    "    print \"min_talk_len (sent based): %2.f\"%min_par_len\n",
    "    \n",
    "    max_par_len = np.max(par_lens)\n",
    "    print \"max_talk_len (sent based): %2.f\"%max_par_len\n",
    "\n",
    "    std_par_len = np.std(par_lens)\n",
    "    print \"std_talk_len (sent based): %2.f\"%std_par_len\n",
    "\n",
    "\n",
    "    \n",
    "    sent_lens = []\n",
    "    for par in ds:\n",
    "        for sent in par:\n",
    "            sent_lens.append(len(sent))\n",
    "    \n",
    "    avg_sent_len = np.average(sent_lens)\n",
    "    print \"avg_sent_len: %.2f\"%avg_sent_len\n",
    "    \n",
    "    min_sent_len = np.min(sent_lens)\n",
    "    print \"min_sent_len: %2.f\"%min_sent_len\n",
    "\n",
    "    max_sent_len = np.max(sent_lens)\n",
    "    print \"max_sent_len: %2.f\"%max_sent_len\n",
    "\n",
    "    std_sent_len = np.std(sent_lens)\n",
    "    print \"std_sent_len: %2.f\"%std_sent_len\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def drawProgressBar(shell_out, \n",
    "                    begin, k, out_of, end, barLen =25):\n",
    "    percent = k/float(out_of)\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        elif i==int(barLen * percent):\n",
    "            progress +='>'\n",
    "        else:\n",
    "            progress += \"_\"\n",
    "    text = \"%s%d/%d[%s](%.2f%%)%s\"%(begin,k,out_of,progress,percent * 100, end)\n",
    "    if shell_out== True:\n",
    "        sys.stdout.write(text)\n",
    "        sys.stdout.flush()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import copy as cp\n",
    "# tmp = cp.deepcopy(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#talks = cp.deepcopy(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1356/1356[=========================](100.00%)"
     ]
    }
   ],
   "source": [
    "for talk_id, talk in talks.items():\n",
    "    text = talk['content']\n",
    "    talks[talk_id]['content'] = pre_process_par(text)\n",
    "    drawProgressBar(True,\"\",talk_id+1,len(talks),\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./ted-gender-annotated/dataset.pkl','wb') as f:\n",
    "    pickle.dump(talks,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size (# talks): 344\n",
      "avg_talk_len: 118.77\n",
      "min_talk_len (sent based): 10\n",
      "max_talk_len (sent based): 267\n",
      "std_talk_len (sent based): 52\n",
      "avg_sent_len: 8.01\n",
      "min_sent_len:  1\n",
      "max_sent_len: 69\n",
      "std_sent_len:  5\n"
     ]
    }
   ],
   "source": [
    "texts = [talk['content'] for talk in talks.values() if talk['gender']=='female']\n",
    "statistics_ds(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size (# talks): 1012\n",
      "avg_talk_len: 133.24\n",
      "min_talk_len (sent based): 11\n",
      "max_talk_len (sent based): 396\n",
      "std_talk_len (sent based): 63\n",
      "avg_sent_len: 7.69\n",
      "min_sent_len:  1\n",
      "max_sent_len: 138\n",
      "std_sent_len:  5\n"
     ]
    }
   ],
   "source": [
    "texts = [talk['content'] for talk in talks.values() if talk['gender']=='male']\n",
    "statistics_ds(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size (# talks): 1356\n",
      "avg_talk_len: 129.57\n",
      "min_talk_len (sent based): 10\n",
      "max_talk_len (sent based): 396\n",
      "std_talk_len (sent based): 61\n",
      "avg_sent_len: 7.76\n",
      "min_sent_len:  1\n",
      "max_sent_len: 138\n",
      "std_sent_len:  5\n"
     ]
    }
   ],
   "source": [
    "texts = [talk['content'] for talk in talks.values()]\n",
    "statistics_ds(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_voc_in_word2vec: 2196016\n",
      "num_voc_in_ds: 327140\n",
      "remove duplicates ... \n",
      "num_voc_in_ds: 27440\n",
      "count_overlap: 25931\n",
      "precent_overlap: 94.50%\n"
     ]
    }
   ],
   "source": [
    "texts = [talk['content'] for talk in talks.values() if talk['gender']=='female']\n",
    "overlap(texts,word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_voc_in_word2vec: 2196016\n",
      "num_voc_in_ds: 1037038\n",
      "remove duplicates ... \n",
      "num_voc_in_ds: 47174\n",
      "count_overlap: 42724\n",
      "precent_overlap: 90.57%\n"
     ]
    }
   ],
   "source": [
    "texts = [talk['content'] for talk in talks.values() if talk['gender']=='male']\n",
    "overlap(texts,word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_voc_in_word2vec: 2196016\n",
      "num_voc_in_ds: 1364178\n",
      "remove duplicates ... \n",
      "num_voc_in_ds: 54317\n",
      "count_overlap: 48459\n",
      "precent_overlap: 89.22%\n"
     ]
    }
   ],
   "source": [
    "texts = [talk['content'] for talk in talks.values()]\n",
    "overlap(texts,word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./ted-gender-annotated/dataset.pkl\",'rb') as f:\n",
    "    talks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of male texts:1012\n",
      "number of female texts:344\n"
     ]
    }
   ],
   "source": [
    "male_texts = []\n",
    "female_texts = []\n",
    "\n",
    "for talk_id, talk in talks.items():\n",
    "    if talk['gender'] == 'male':\n",
    "        male_texts.append(talk['content'])\n",
    "    else:\n",
    "        female_texts.append(talk['content'])\n",
    "print \"number of male texts:%d\"%len(male_texts)\n",
    "print \"number of female texts:%d\"%len(female_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'ocean', u'thing', u'complicated', u'can'],\n",
       " [u'thing', u'complicated', u'human', u'health', u'can'],\n",
       " [u'daunting',\n",
       "  u'simple',\n",
       "  u'move',\n",
       "  u'themes',\n",
       "  u'say',\n",
       "  u'seem',\n",
       "  u'even',\n",
       "  u'two',\n",
       "  u'going',\n",
       "  u'forward',\n",
       "  u'really',\n",
       "  u'might',\n",
       "  u'understand',\n",
       "  u'bringing',\n",
       "  u'task',\n",
       "  u'together',\n",
       "  u'try',\n",
       "  u'complexity',\n",
       "  u'can',\n",
       "  u'think'],\n",
       " [u'simple',\n",
       "  u'themes',\n",
       "  u'really',\n",
       "  u'aren',\n",
       "  u'things',\n",
       "  u'complex',\n",
       "  u'going',\n",
       "  u'pretty',\n",
       "  u'know',\n",
       "  u'science',\n",
       "  u'well'],\n",
       " [u'nobody', u'ain', u'one', u'start', u'momma', u'going', u'happy'],\n",
       " [u'right', u'experienced', u'know'],\n",
       " [u'just',\n",
       "  u'go',\n",
       "  u'next',\n",
       "  u'take',\n",
       "  u'build',\n",
       "  u'nobody',\n",
       "  u'step',\n",
       "  u'ocean',\n",
       "  u'ain',\n",
       "  u'can',\n",
       "  u'happy'],\n",
       " [u'theme', u'talk'],\n",
       " [u'lot', u'ways', u'different', u'unhappy', u'ocean', u'pretty', u'making'],\n",
       " [u'shot', u'cannery', u'1932', u'row'],\n",
       " [u'industrial',\n",
       "  u'west',\n",
       "  u'cannery',\n",
       "  u'coast',\n",
       "  u'time',\n",
       "  u'biggest',\n",
       "  u'canning',\n",
       "  u'operation',\n",
       "  u'row'],\n",
       " [u'amounts', u'enormous', u'air', u'water', u'piled', u'pollution'],\n",
       " [u'turned',\n",
       "  u'bolin',\n",
       "  u'fumes',\n",
       "  u'floating',\n",
       "  u'lead-based',\n",
       "  u'paints',\n",
       "  u'station',\n",
       "  u'black',\n",
       "  u'rolf',\n",
       "  u'marine',\n",
       "  u'1940s',\n",
       "  u'professor',\n",
       "  u'work',\n",
       "  u'bay',\n",
       "  u'bad',\n",
       "  u'hopkin',\n",
       "  u'scum',\n",
       "  u'wrote',\n",
       "  u'inlets'],\n",
       " [u'saying',\n",
       "  u'people',\n",
       "  u'canneries',\n",
       "  u'working',\n",
       "  u'smell',\n",
       "  u'stay',\n",
       "  u'know',\n",
       "  u'day',\n",
       "  u'barely',\n",
       "  u'came'],\n",
       " [u'say', u'know', u'smell'],\n",
       " [u'money', u'smell'],\n",
       " [u'people',\n",
       "  u'money',\n",
       "  u'community',\n",
       "  u'skin',\n",
       "  u'needed',\n",
       "  u'absorbed',\n",
       "  u'dealt',\n",
       "  u'bodies',\n",
       "  u'pollution'],\n",
       " [u'made', u'unhealthy', u'people', u'unhappy', u'ocean'],\n",
       " [u'minnow',\n",
       "  u'want',\n",
       "  u'simple',\n",
       "  u'adages',\n",
       "  u'another',\n",
       "  u'based',\n",
       "  u'actually',\n",
       "  u'health',\n",
       "  u'call',\n",
       "  u'couple',\n",
       "  u'human',\n",
       "  u'upon',\n",
       "  u'hurt',\n",
       "  u'whale',\n",
       "  u'pinch',\n",
       "  u'ocean',\n",
       "  u'connection'],\n",
       " [u'...', u'life', u'pyramid', u'ocean'],\n",
       " [u'see',\n",
       "  u'looks',\n",
       "  u'different',\n",
       "  u'things',\n",
       "  u'way',\n",
       "  u'tell',\n",
       "  u'interconnections',\n",
       "  u'ecologist',\n",
       "  u'regular',\n",
       "  u'now',\n",
       "  u'look',\n",
       "  u'ocean',\n",
       "  u'person'],\n",
       " [u'pyramid',\n",
       "  u'chain',\n",
       "  u'middle',\n",
       "  u'see',\n",
       "  u'things',\n",
       "  u'food',\n",
       "  u'diagram',\n",
       "  u'base',\n",
       "  u'animals',\n",
       "  u'plankton',\n",
       "  u'small'],\n",
       " [u'life', u'see', u'top', u'flow', u'ecologists', u'base'],\n",
       " [u'preserve', u'save', u'heal', u'ocean', u'say', u'trying'],\n",
       " [u'pyramid'],\n",
       " [u'matter', u'health', u'human', u'now'],\n",
       " [u'bottom',\n",
       "  u'things',\n",
       "  u'shouldn',\n",
       "  u'jam',\n",
       "  u'pyramid',\n",
       "  u'frightening',\n",
       "  u'happen'],\n",
       " [u'molecules',\n",
       "  u'pcbs',\n",
       "  u'created',\n",
       "  u'can',\n",
       "  u'broken',\n",
       "  u'us',\n",
       "  u'pollutants',\n",
       "  u'bodies',\n",
       "  u'like'],\n",
       " [u'pyramid',\n",
       "  u'go',\n",
       "  u'top',\n",
       "  u'passed',\n",
       "  u'way',\n",
       "  u'drift',\n",
       "  u'base',\n",
       "  u'accumulate',\n",
       "  u'predators'],\n",
       " [u'little', u'bring', u'invent', u'thought', u'game', u'home', u'now'],\n",
       " [u'don', u'just', u'play', u'can', u'think', u'really'],\n",
       " [u'chocolate', u'game', u'styrofoam'],\n",
       " [u'peanuts', u'two', u'imagine', u'got', u'given', u'styrofoam', u'boat'],\n",
       " [u'pocket', u'much', u'can', u'put'],\n",
       " [u'somebody',\n",
       "  u'give',\n",
       "  u'peanuts',\n",
       "  u'offer',\n",
       "  u'rules',\n",
       "  u'drink',\n",
       "  u'styrofoam',\n",
       "  u'every',\n",
       "  u'time',\n",
       "  u'suppose'],\n",
       " [u'people',\n",
       "  u'society',\n",
       "  u'happen',\n",
       "  u'start',\n",
       "  u'stingiest',\n",
       "  u'drunkest',\n",
       "  u'peanuts',\n",
       "  u'moving',\n",
       "  u'accumulate',\n",
       "  u'styrofoam',\n",
       "  u'will'],\n",
       " [u'go',\n",
       "  u'indigestible',\n",
       "  u'mechanism',\n",
       "  u'pile',\n",
       "  u'peanuts',\n",
       "  u'game',\n",
       "  u'styrofoam',\n",
       "  u'bigger',\n",
       "  u'anywhere'],\n",
       " [u'happens', u'pyramid', u'exactly', u'pdbs', u'food', u'accumulate', u'top'],\n",
       " [u'lovely',\n",
       "  u'chocolates',\n",
       "  u'little',\n",
       "  u'suppose',\n",
       "  u'take',\n",
       "  u'peanuts',\n",
       "  u'instead',\n",
       "  u'get',\n",
       "  u'now',\n",
       "  u'styrofoam'],\n",
       " [u'just',\n",
       "  u'one',\n",
       "  u'accumulate',\n",
       "  u'pass',\n",
       "  u'group',\n",
       "  u'chocolates',\n",
       "  u'absorbed',\n",
       "  u'accumulating',\n",
       "  u'instead',\n",
       "  u'eating',\n",
       "  u'around',\n",
       "  u'well',\n",
       "  u'us',\n",
       "  u'will',\n",
       "  u'passing'],\n",
       " [u'natural',\n",
       "  u'like',\n",
       "  u'omega-3',\n",
       "  u'food',\n",
       "  u'chain',\n",
       "  u'say',\n",
       "  u'something',\n",
       "  u'want',\n",
       "  u'pcb',\n",
       "  u'difference',\n",
       "  u'marine'],\n",
       " [u'accumulate', u'pcbs'],\n",
       " [u'unfortunately', u'great', u'examples'],\n",
       " [u'north',\n",
       "  u'pcbs',\n",
       "  u'sarasota',\n",
       "  u'bay',\n",
       "  u'accumulate',\n",
       "  u'carolina',\n",
       "  u'texas',\n",
       "  u'dolphins'],\n",
       " [u'chain', u'get', u'food'],\n",
       " [u'pcbs',\n",
       "  u'fish',\n",
       "  u'plankton',\n",
       "  u'accumulate',\n",
       "  u'fat-soluble',\n",
       "  u'eat',\n",
       "  u'dolphins'],\n",
       " [u'dolphin', u'one', u'can', u'way', u'mother', u'pcb', u'get', u'now'],\n",
       " [u'mother', u'milk'],\n",
       " [u'load', u'sarasota', u'bay', u'diagram', u'pcb', u'dolphins'],\n",
       " [u'load', u'huge', u'males', u'adult'],\n",
       " [u'huge', u'juveniles', u'load'],\n",
       " [u'load', u'lower', u'already', u'females', u'weaned', u'calf', u'first'],\n",
       " [u'females', u'trying'],\n",
       " [u'survive',\n",
       "  u'females',\n",
       "  u'pcbs',\n",
       "  u'mother',\n",
       "  u'fat',\n",
       "  u'milk',\n",
       "  u'offspring',\n",
       "  u'don',\n",
       "  u'passing'],\n",
       " [u'death',\n",
       "  u'dolphin',\n",
       "  u'percent',\n",
       "  u'60',\n",
       "  u'born',\n",
       "  u'rate',\n",
       "  u'every',\n",
       "  u'female',\n",
       "  u'first',\n",
       "  u'80',\n",
       "  u'calf',\n",
       "  u'dolphins'],\n",
       " [u'full', u'mothers', u'pump', u'die', u'offspring', u'pollutant', u'first'],\n",
       " [u'go',\n",
       "  u'death',\n",
       "  u'pay',\n",
       "  u'calf',\n",
       "  u'price',\n",
       "  u'first-born',\n",
       "  u'now',\n",
       "  u'pollutant',\n",
       "  u'animals',\n",
       "  u'terrible',\n",
       "  u'reproduce',\n",
       "  u'can',\n",
       "  u'mother',\n",
       "  u'accumulation'],\n",
       " [u'turns', u'top', u'predator', u'ocean', u'another'],\n",
       " [u'top', u'predator', u'us', u'course'],\n",
       " [u'eating', u'meat', u'places', u'also', u'comes'],\n",
       " [u'grocery', u'meat', u'tokyo', u'whale', u'store', u'photographed'],\n",
       " [u'years',\n",
       "  u'really',\n",
       "  u'identify',\n",
       "  u'dna',\n",
       "  u'samples',\n",
       "  u'biology',\n",
       "  u'molecular',\n",
       "  u'ago',\n",
       "  u'use',\n",
       "  u'lab',\n",
       "  u'genetically',\n",
       "  u'test',\n",
       "  u'whale',\n",
       "  u'meat',\n",
       "  u'tokyo',\n",
       "  u'learn',\n",
       "  u'smuggle',\n",
       "  u'fact'],\n",
       " [u'meat', u'samples', u'whale'],\n",
       " [u'meat', u'way', u'illegal', u'whale'],\n",
       " [u'story', u'another'],\n",
       " [u'meat', u'whale'],\n",
       " [u'even', u'meat', u'though', u'dolphin', u'labeled', u'whale'],\n",
       " [u'dolphin', u'liver', u'blubber'],\n",
       " [u'pcbs',\n",
       "  u'huge',\n",
       "  u'dolphin',\n",
       "  u'load',\n",
       "  u'parts',\n",
       "  u'metals',\n",
       "  u'dioxins',\n",
       "  u'heavy'],\n",
       " [u'huge', u'ate', u'meat', u'people', u'load', u'passing'],\n",
       " [u'meat',\n",
       "  u'turns',\n",
       "  u'sold',\n",
       "  u'around',\n",
       "  u'lot',\n",
       "  u'world',\n",
       "  u'dolphins',\n",
       "  u'whale',\n",
       "  u'market'],\n",
       " [u'eating',\n",
       "  u'meat',\n",
       "  u'populations',\n",
       "  u'people',\n",
       "  u'also',\n",
       "  u'know',\n",
       "  u'toxic',\n",
       "  u'don',\n",
       "  u'tragedy'],\n",
       " [u'ago', u'years', u'data'],\n",
       " [u'dolphin',\n",
       "  u'really',\n",
       "  u'sitting',\n",
       "  u'toxic',\n",
       "  u'markets',\n",
       "  u'knew',\n",
       "  u'sold',\n",
       "  u'desk',\n",
       "  u'world',\n",
       "  u'whale',\n",
       "  u'meat',\n",
       "  u'remember',\n",
       "  u'person'],\n",
       " [u'times',\n",
       "  u'allowed',\n",
       "  u'toxic',\n",
       "  u'epa',\n",
       "  u'ever',\n",
       "  u'loads',\n",
       "  u'two-to-three-to-400'],\n",
       " [u'thinking',\n",
       "  u'discovery',\n",
       "  u'scientific',\n",
       "  u'sitting',\n",
       "  u'know',\n",
       "  u'desk',\n",
       "  u'awful',\n",
       "  u'great',\n",
       "  u'remember',\n",
       "  u'well'],\n",
       " [u'protocol',\n",
       "  u'begin',\n",
       "  u'journals',\n",
       "  u'publish',\n",
       "  u'take',\n",
       "  u'scientific',\n",
       "  u'broke',\n",
       "  u'data',\n",
       "  u'career',\n",
       "  u'time',\n",
       "  u'talk',\n",
       "  u'first'],\n",
       " [u'breastfeeding',\n",
       "  u'people',\n",
       "  u'pointed',\n",
       "  u'simply',\n",
       "  u'something',\n",
       "  u'japan',\n",
       "  u'children',\n",
       "  u'young',\n",
       "  u'health',\n",
       "  u'toxic',\n",
       "  u'really',\n",
       "  u'sent',\n",
       "  u'may',\n",
       "  u'polite',\n",
       "  u'minister',\n",
       "  u'letter',\n",
       "  u'buying',\n",
       "  u'mothers',\n",
       "  u'us',\n",
       "  u'thought',\n",
       "  u'healthy',\n",
       "  u'situation',\n",
       "  u'intolerable'],\n",
       " [u'point',\n",
       "  u'series',\n",
       "  u'say',\n",
       "  u'selling',\n",
       "  u'japan',\n",
       "  u'still',\n",
       "  u'really',\n",
       "  u'even',\n",
       "  u'though',\n",
       "  u'incorrectly',\n",
       "  u'difficult',\n",
       "  u'buy',\n",
       "  u'led',\n",
       "  u'campaigns',\n",
       "  u'shouldn',\n",
       "  u'whale',\n",
       "  u'believe',\n",
       "  u'meat',\n",
       "  u'anything',\n",
       "  u'proud',\n",
       "  u'labeled',\n",
       "  u'whole'],\n",
       " [u'longer',\n",
       "  u'dolphin',\n",
       "  u'least',\n",
       "  u'labeled',\n",
       "  u'going',\n",
       "  u'correctly',\n",
       "  u'toxic',\n",
       "  u'instead',\n",
       "  u'buying',\n",
       "  u'meat'],\n",
       " [u'just',\n",
       "  u'seals',\n",
       "  u'states',\n",
       "  u'ended',\n",
       "  u'happens',\n",
       "  u'united',\n",
       "  u'pcbs',\n",
       "  u'whales',\n",
       "  u'parts',\n",
       "  u'gathered',\n",
       "  u'european',\n",
       "  u'arctic',\n",
       "  u'diet',\n",
       "  u'communities',\n",
       "  u'world',\n",
       "  u'women',\n",
       "  u'leads',\n",
       "  u'natural',\n",
       "  u'canadian',\n",
       "  u'isn',\n",
       "  u'accumulation'],\n",
       " [u'breast', u'toxic', u'milk', u'women'],\n",
       " [u'feed',\n",
       "  u'chain',\n",
       "  u'pyramid',\n",
       "  u'milk',\n",
       "  u'children',\n",
       "  u'food',\n",
       "  u'part',\n",
       "  u'breast',\n",
       "  u'world',\n",
       "  u'toxins',\n",
       "  u'ocean',\n",
       "  u'can',\n",
       "  u'offspring',\n",
       "  u'accumulation'],\n",
       " [u'compromised', u'means', u'immune', u'systems'],\n",
       " [u'development', u'means', u'can', u'children', u'compromised'],\n",
       " [u'pyramid',\n",
       "  u'particularly',\n",
       "  u'world',\n",
       "  u'decade',\n",
       "  u'eat',\n",
       "  u'attention',\n",
       "  u'changing',\n",
       "  u'women',\n",
       "  u'last',\n",
       "  u'problem',\n",
       "  u'reduced'],\n",
       " [u'pyramid', u'natural', u'solve', u'taken', u'problem', u'order'],\n",
       " [u'acute',\n",
       "  u'good',\n",
       "  u'pyramid',\n",
       "  u'thing',\n",
       "  u'solve',\n",
       "  u'particular',\n",
       "  u'nothing',\n",
       "  u'problem'],\n",
       " [u'breaking', u'ways', u'pyramid'],\n",
       " [u'pyramid',\n",
       "  u'like',\n",
       "  u'bottom',\n",
       "  u'things',\n",
       "  u'get',\n",
       "  u'sewer',\n",
       "  u'jam',\n",
       "  u'can',\n",
       "  u'backed',\n",
       "  u'clogged',\n",
       "  u'line'],\n",
       " [u'pyramid',\n",
       "  u'food',\n",
       "  u'nutrients',\n",
       "  u'back',\n",
       "  u'jam',\n",
       "  u'can',\n",
       "  u'base',\n",
       "  u'fertilizer',\n",
       "  u'sewage'],\n",
       " [u'heard',\n",
       "  u'floating',\n",
       "  u'end',\n",
       "  u'things',\n",
       "  u'damage',\n",
       "  u'causing',\n",
       "  u'toxic',\n",
       "  u'blooms',\n",
       "  u'tides',\n",
       "  u'oceans',\n",
       "  u'neurological',\n",
       "  u'algae',\n",
       "  u'red',\n",
       "  u'example'],\n",
       " [u'ocean', u'also', u'see', u'viruses', u'can', u'bacteria', u'blooms'],\n",
       " [u'tide',\n",
       "  u'bacteria',\n",
       "  u'vibrio',\n",
       "  u'two',\n",
       "  u'red',\n",
       "  u'cholera',\n",
       "  u'includes',\n",
       "  u'coming',\n",
       "  u'shore',\n",
       "  u'shots',\n",
       "  u'genus'],\n",
       " [u'closed', u'people', u'many', u'sign', u'seen', u'beach'],\n",
       " [u'happen'],\n",
       " [u'overfill',\n",
       "  u'pyramid',\n",
       "  u'jammed',\n",
       "  u'bacteria',\n",
       "  u'happens',\n",
       "  u'much',\n",
       "  u'clog',\n",
       "  u'base',\n",
       "  u'beaches',\n",
       "  u'natural',\n",
       "  u'ocean',\n",
       "  u'onto'],\n",
       " [u'often', u'jams', u'us', u'sewage'],\n",
       " [u'gone',\n",
       "  u'human',\n",
       "  u'saying',\n",
       "  u'national',\n",
       "  u'sign',\n",
       "  u'use',\n",
       "  u'state',\n",
       "  u'closed',\n",
       "  u'ever',\n",
       "  u'far',\n",
       "  u'big',\n",
       "  u'park',\n",
       "  u'front',\n",
       "  u'now',\n",
       "  u'can',\n",
       "  u'many',\n",
       "  u'sewage'],\n",
       " [u'wouldn', u'often', u'tolerate'],\n",
       " [u'beaches',\n",
       "  u'wouldn',\n",
       "  u'closed',\n",
       "  u'country',\n",
       "  u'tolerate',\n",
       "  u'parks',\n",
       "  u'swamped',\n",
       "  u'lot',\n",
       "  u'human',\n",
       "  u'sewage'],\n",
       " [u'shouldn',\n",
       "  u'around',\n",
       "  u'tolerate',\n",
       "  u'reason',\n",
       "  u'either',\n",
       "  u'closed',\n",
       "  u'world',\n",
       "  u'believe'],\n",
       " [u'organisms',\n",
       "  u'just',\n",
       "  u'cleanliness',\n",
       "  u'question',\n",
       "  u'disease',\n",
       "  u'also',\n",
       "  u'human',\n",
       "  u'turn'],\n",
       " [u'vibrios', u'people', u'infect', u'actually', u'can', u'bacteria'],\n",
       " [u'infections', u'can', u'skin', u'go', u'create'],\n",
       " [u'people',\n",
       "  u'noaa',\n",
       "  u'initiative',\n",
       "  u'human',\n",
       "  u'vibrio',\n",
       "  u'graph',\n",
       "  u'health',\n",
       "  u'showing',\n",
       "  u'rise',\n",
       "  u'years',\n",
       "  u'last',\n",
       "  u'infections',\n",
       "  u'ocean'],\n",
       " [u'surfers', u'know', u'incredibly', u'example'],\n",
       " [u'poo',\n",
       "  u'see',\n",
       "  u'weather',\n",
       "  u'sites',\n",
       "  u'rider',\n",
       "  u'surf',\n",
       "  u'little',\n",
       "  u'alert',\n",
       "  u'waves',\n",
       "  u'flashing',\n",
       "  u'surfing',\n",
       "  u'like',\n",
       "  u'can',\n",
       "  u'fact'],\n",
       " [u'legacy',\n",
       "  u'carry',\n",
       "  u'might',\n",
       "  u'even',\n",
       "  u'long',\n",
       "  u'take',\n",
       "  u'time',\n",
       "  u'solve',\n",
       "  u'beach',\n",
       "  u'means',\n",
       "  u'infection',\n",
       "  u'waves',\n",
       "  u'day',\n",
       "  u'great',\n",
       "  u'surfing',\n",
       "  u'dangerous',\n",
       "  u'surfers',\n",
       "  u'place',\n",
       "  u'can'],\n",
       " [u'even',\n",
       "  u'now',\n",
       "  u'antibiotic',\n",
       "  u'genes',\n",
       "  u'infections',\n",
       "  u'actually',\n",
       "  u'carrying',\n",
       "  u'difficult',\n",
       "  u'resistance',\n",
       "  u'makes'],\n",
       " [u'create', u'infections', u'harmful', u'algal', u'blooms'],\n",
       " [u'generating', u'kinds', u'chemicals', u'blooms'],\n",
       " [u'just',\n",
       "  u'simple',\n",
       "  u'poisoningfish',\n",
       "  u'want',\n",
       "  u'neurotoxic',\n",
       "  u'ciguatera',\n",
       "  u'shellfish',\n",
       "  u'paralytic',\n",
       "  u'blooms',\n",
       "  u'poisons',\n",
       "  u'harmful',\n",
       "  u'algal',\n",
       "  u'know',\n",
       "  u'come',\n",
       "  u'types',\n",
       "  u'don',\n",
       "  u'list',\n",
       "  u'diarrheic',\n",
       "  u'poisoning'],\n",
       " [u'chain', u'getting', u'things', u'food', u'blooms'],\n",
       " [u'calwell',\n",
       "  u'brought',\n",
       "  u'communities',\n",
       "  u'human',\n",
       "  u'story',\n",
       "  u'interesting',\n",
       "  u'copepod',\n",
       "  u'rita',\n",
       "  u'normal',\n",
       "  u'cholera',\n",
       "  u'famously',\n",
       "  u'marine',\n",
       "  u'traced',\n",
       "  u'vector'],\n",
       " [u'small', u'crustaceans', u'copepods'],\n",
       " [u'leads',\n",
       "  u'long',\n",
       "  u'human',\n",
       "  u'carry',\n",
       "  u'bacteria',\n",
       "  u'little',\n",
       "  u'tiny',\n",
       "  u'fraction',\n",
       "  u'inch',\n",
       "  u'legs',\n",
       "  u'cholera',\n",
       "  u'disease',\n",
       "  u'can'],\n",
       " [u'move',\n",
       "  u'doesn',\n",
       "  u'concentration',\n",
       "  u'around',\n",
       "  u'make',\n",
       "  u'sure',\n",
       "  u'led',\n",
       "  u'increased',\n",
       "  u'cholera',\n",
       "  u'sparked',\n",
       "  u'world',\n",
       "  u'along',\n",
       "  u'trying',\n",
       "  u'epidemics',\n",
       "  u'vectors',\n",
       "  u'shipping',\n",
       "  u'ports'],\n",
       " [u'major',\n",
       "  u'pyramid',\n",
       "  u'flow',\n",
       "  u'blocked',\n",
       "  u'working',\n",
       "  u'may',\n",
       "  u'problems',\n",
       "  u'disrupted',\n",
       "  u'base',\n",
       "  u'clogged',\n",
       "  u'ecosystem',\n",
       "  u'well'],\n",
       " [u'sort', u'flow', u'disrupted'],\n",
       " [u'well', u'things', u'bunch'],\n",
       " [u'plumber', u'example', u'call', u'joe'],\n",
       " [u'flow', u'come', u'fix'],\n",
       " [u'begun',\n",
       "  u'people',\n",
       "  u'issues',\n",
       "  u'fix',\n",
       "  u'able',\n",
       "  u'spots',\n",
       "  u'hope',\n",
       "  u'around',\n",
       "  u'may',\n",
       "  u'problems',\n",
       "  u'world',\n",
       "  u'come',\n",
       "  u'grips',\n",
       "  u'look',\n",
       "  u'places',\n",
       "  u'turn',\n",
       "  u'fixed',\n",
       "  u'fact'],\n",
       " [u'monterey', u'one'],\n",
       " [u'monterey',\n",
       "  u'much',\n",
       "  u'started',\n",
       "  u'showing',\n",
       "  u'problems',\n",
       "  u'attendant',\n",
       "  u'ecosystem',\n",
       "  u'industry',\n",
       "  u'bay',\n",
       "  u'distressed',\n",
       "  u'canning',\n",
       "  u'pollution'],\n",
       " [u'picture', u'1932'],\n",
       " [u'picture', u'different', u'2009', u'dramatically'],\n",
       " [u'gone', u'abated', u'canneries', u'pollution'],\n",
       " [u'greater',\n",
       "  u'working',\n",
       "  u'need',\n",
       "  u'individual',\n",
       "  u'sense',\n",
       "  u'communities',\n",
       "  u'ecosystems'],\n",
       " [u'pyramid', u'way', u'top', u'base', u'need', u'functioning'],\n",
       " [u'monterey',\n",
       "  u'pyramid',\n",
       "  u'people',\n",
       "  u'efforts',\n",
       "  u'years',\n",
       "  u'right',\n",
       "  u'functioned',\n",
       "  u'functioning',\n",
       "  u'150',\n",
       "  u'different',\n",
       "  u'better',\n",
       "  u'lot',\n",
       "  u'ever',\n",
       "  u'now',\n",
       "  u'last'],\n",
       " [u'happen', u'didn', u'accidentally'],\n",
       " [u'pioneering',\n",
       "  u'people',\n",
       "  u'many',\n",
       "  u'time',\n",
       "  u'put',\n",
       "  u'effort',\n",
       "  u'spirit',\n",
       "  u'happened'],\n",
       " [u'little',\n",
       "  u'grove',\n",
       "  u'julia',\n",
       "  u'platt',\n",
       "  u'pacific',\n",
       "  u'hometown',\n",
       "  u'mayor',\n",
       "  u'left'],\n",
       " [u'protect',\n",
       "  u'old',\n",
       "  u'years',\n",
       "  u'became',\n",
       "  u'done',\n",
       "  u'ocean',\n",
       "  u'mayor',\n",
       "  u'74',\n",
       "  u'something'],\n",
       " [u'gone',\n",
       "  u'right',\n",
       "  u'community-based',\n",
       "  u'julia',\n",
       "  u'produced',\n",
       "  u'canneries',\n",
       "  u'seed',\n",
       "  u'area',\n",
       "  u'provide',\n",
       "  u'next',\n",
       "  u'polluting',\n",
       "  u'biggest',\n",
       "  u'grow',\n",
       "  u'eventually',\n",
       "  u'knew',\n",
       "  u'1931',\n",
       "  u'california',\n",
       "  u'spark',\n",
       "  u'wanted',\n",
       "  u'marine',\n",
       "  u'needed',\n",
       "  u'cannery',\n",
       "  u'ocean',\n",
       "  u'protected',\n",
       "  u'place',\n",
       "  u'first'],\n",
       " [u'monterey',\n",
       "  u'just',\n",
       "  u'people',\n",
       "  u'lock',\n",
       "  u'julie',\n",
       "  u'david',\n",
       "  u'area',\n",
       "  u'bay',\n",
       "  u'health',\n",
       "  u'economy',\n",
       "  u'eating',\n",
       "  u'ecosystem',\n",
       "  u'instrumental',\n",
       "  u'important',\n",
       "  u'aquarium',\n",
       "  u'producing',\n",
       "  u'like',\n",
       "  u'packard',\n",
       "  u'ocean',\n",
       "  u'notion'],\n",
       " [u'monterey',\n",
       "  u'fortunes',\n",
       "  u'thinking',\n",
       "  u'led',\n",
       "  u'around',\n",
       "  u'dramatic',\n",
       "  u'world',\n",
       "  u'change',\n",
       "  u'places',\n",
       "  u'shift',\n",
       "  u'bay'],\n",
       " [u'pyramid',\n",
       "  u'life',\n",
       "  u'want',\n",
       "  u'really',\n",
       "  u'leave',\n",
       "  u'connects',\n",
       "  u'trying',\n",
       "  u'protect',\n",
       "  u'well',\n",
       "  u'ocean',\n",
       "  u'thought'],\n",
       " [u'pyramid',\n",
       "  u'life',\n",
       "  u'intricately',\n",
       "  u'terrestrial',\n",
       "  u'species',\n",
       "  u'lives',\n",
       "  u'land',\n",
       "  u'ocean',\n",
       "  u'planet',\n",
       "  u'connected',\n",
       "  u'think'],\n",
       " [u'ocean', u'healthy', u'can', u'remain'],\n",
       " [u'much', u'thank']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
