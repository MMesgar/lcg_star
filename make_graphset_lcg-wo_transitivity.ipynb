{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import general packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def voc_count(corpus):\n",
    "    d = defaultdict(int)\n",
    "    for p in corpus:\n",
    "        for sent in p:\n",
    "            for t in sent:\n",
    "                d[t] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#load list of stop words\n",
    "with open('./snowball_stopwords.txt','rb') as sw:\n",
    "    stop_words = [line.strip() for line in sw]\n",
    "\n",
    "#load punctuations \n",
    "punctuations = string.punctuation\n",
    "\n",
    "#extra characters\n",
    "extra = []\n",
    "\n",
    "def pre_process_par(par):\n",
    "    \"\"\"\n",
    "    input: \n",
    "       a paragraph\n",
    "    output:\n",
    "       list of sentences. Each sentence is a list of tokens.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # make the par lowecase\n",
    "    par = par.lower()\n",
    "    \n",
    "    # split sentences\n",
    "    sent_par = sent_tokenize(par)\n",
    "    \n",
    "    # tokenize and clean all sentences\n",
    "    for sent in sent_par:\n",
    "        \n",
    "        #tokenize each sentence\n",
    "        tokens = word_tokenize(sent)\n",
    "        \n",
    "        # remove repetitve words in a sentenece\n",
    "        tokens = list(set(tokens))\n",
    "        \n",
    "        #remove stop words and clean texts\n",
    "        tokens = [tok for tok in tokens if \n",
    "                                          (tok not in stop_words) and \n",
    "                                          (tok not in punctuations) and \n",
    "                                          (tok not in extra)]\n",
    "        \n",
    "        # put it in the output\n",
    "        output.append(tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_path = './glove.840B.300d.txt'\n",
    "\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(seed=1)\n",
    "\n",
    "cn = 0\n",
    "word2vec = {}\n",
    "with open(w2v_path,'rb') as w2v:\n",
    "    content = w2v.read().strip()\n",
    "    for line in content.split('\\n'):\n",
    "        cn +=1\n",
    "        line = line.strip().split()\n",
    "        v = line[0]\n",
    "        \n",
    "        vector = line[1:]\n",
    "        vector = np.matrix(vector,dtype='float32')\n",
    "        \n",
    "        word2vec[v] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def apply_rule4(graph):\n",
    "    #Rule 4\n",
    "    adj_list= deepcopy(graph)\n",
    "    for sent_id, edges in enumerate(adj_list):\n",
    "        for edge in edges:\n",
    "            if edge[-1] == 'trans':\n",
    "                continue\n",
    "            target_sent_id = edge[0]\n",
    "            source_word = edge[1]\n",
    "            target_word = edge[2]\n",
    "            if target_sent_id != -1:\n",
    "                trans_source_id = sent_id\n",
    "                trans_source_word = source_word\n",
    "                trans_weight = 'trans'\n",
    "               \n",
    "                j_edges = adj_list[target_sent_id]\n",
    "                tmp = [e for e in j_edges if e[1]==target_word]\n",
    "                for item in tmp:\n",
    "                    trans_target_id = item[0]\n",
    "                    trans_target_word = item[2]\n",
    "                    trans_edge = (trans_target_id, trans_source_word, trans_target_word, trans_weight)\n",
    "                    adj_list[sent_id].append(trans_edge)\n",
    "\n",
    "    return adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from scipy import spatial\n",
    "def create_graph(list_of_sents):\n",
    "    \n",
    "    # get the number of sentences or nodes\n",
    "    n = len(list_of_sents)\n",
    "    \n",
    "    #initialize the adjacent list for the graph representation of this text\n",
    "    adj_list = [[(-1,None,None, 0.0)]]*n\n",
    "    max_weight = [0.0]*n\n",
    "\n",
    "    #for each sentence check the similarity of each word with all previous words of all connections\n",
    "    word_connections = dict()\n",
    "    \n",
    "    for i in range(1,n):            \n",
    "            sent_i = list_of_sents[i]\n",
    "                                 \n",
    "            for w in sent_i:\n",
    "                \n",
    "                word_connections[(i,w)] = (-1.0, None, -1.0)\n",
    "                try:\n",
    "                    w_vec = word2vec[w]\n",
    "                except:\n",
    "                    word2vec[w] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "                    w_vec = word2vec[w]\n",
    "                \n",
    "                #w_vec = get_word_vector(w)    \n",
    "                for j, sent_j in enumerate(list_of_sents[:i]):\n",
    "                    for v in sent_j:\n",
    "                        try:\n",
    "                            v_vec = word2vec[v]\n",
    "                        except:\n",
    "                            word2vec[v] = rng.uniform(low=-0.2, high=+0.2, size=(300,))\n",
    "                            v_vec = word2vec[v]\n",
    "                            \n",
    "                        #v_vec = get_word_vector(v)\n",
    "                        cosine_w_v =  np.abs(1 - spatial.distance.cosine(w_vec, v_vec))\n",
    "                        cosine_w_v = np.round(cosine_w_v,4)\n",
    "                        if cosine_w_v >= word_connections[(i,w)][2]: #Rule 1\n",
    "                            word_connections[(i,w)] = (j, v, cosine_w_v)\n",
    "                # Rule 2\n",
    "                if word_connections[(i,w)][2] > max_weight[i]:\n",
    "                    adj_list[i] = [(word_connections[(i,w)][0],\n",
    "                                    w,\n",
    "                                    word_connections[(i,w)][1],\n",
    "                                    word_connections[(i,w)][2])]\n",
    "                    max_weight[i] = word_connections[(i,w)][2]\n",
    "                # Rule 3\n",
    "                elif word_connections[(i,w)][2] == max_weight[i]:\n",
    "                    flag = False\n",
    "                    for item in adj_list[i]:\n",
    "                        if item[0] == word_connections[(i,w)][0]:\n",
    "                            print \"here\"\n",
    "                            flag = True\n",
    "                            break\n",
    "                    if flag == False:\n",
    "                        adj_list[i].append(\n",
    "                            (word_connections[(i,w)][0],\n",
    "                             w,\n",
    "                             word_connections[(i,w)][1],\n",
    "                             word_connections[(i,w)][2]))\n",
    "    #Rule 4\n",
    "    #adj_list = apply_rule4(adj_list)\n",
    "    return adj_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(graph_set, path):\n",
    "    output_content = []\n",
    "    for graph_name,  adj_list in enumerate(graph_set):\n",
    "        output_content.append('XP')\n",
    "        output_content.append('% '+str(graph_name))\n",
    "        #output_content.append('t # %d'%graph_name)\n",
    "        num_nodes = len(adj_list)\n",
    "\n",
    "        for n in range(num_nodes):\n",
    "            output_content.append('v %s a'%str(n))\n",
    "\n",
    "        for i, edges in enumerate(adj_list):\n",
    "            if i>0:\n",
    "                for j in edges:\n",
    "                    #Note: we computed edges backward, but we should \n",
    "                    # save them forward to be compatible with NAACL16\n",
    "                    source = j[0] \n",
    "                    target = i\n",
    "                    if target > source:\n",
    "                        if source>0 and target>0:\n",
    "                            output_content.append('d %s %s 1'%(str(source),str(target)))\n",
    "                    else:\n",
    "                        raise ValueError(\"Backward eadge?\")\n",
    "\n",
    "    with open(path,'wb') as out:\n",
    "        out.write('\\n'.join(output_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('./Hansard/hansard.en.original.10K.out', 'rb','utf8') as orig:\n",
    "    orig_paragraphs = []\n",
    "    for line in orig:\n",
    "        if line != '\\n' and len(line)>0:\n",
    "            orig_paragraphs.append(line.strip())\n",
    "print \"# paragraphs in original: %d\"%len(orig_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "original_pars = []\n",
    "original_pars = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(pre_process_par), orig_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "graphs = []\n",
    "graphs = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "                 map(delayed(create_graph), original_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('original_pars_graphs.pickle','wb') as out:\n",
    "    pickle.dump(graphs, out)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(graphs,'./orig_graph_set.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open('./Hansard/hansard.en.translated.from.fr.10K.out','rb','utf8') as tran:\n",
    "    tran_paragraphs = []\n",
    "    for line in tran:\n",
    "        if line != '\\n' and len(line)>0:\n",
    "            tran_paragraphs.append(line.strip())\n",
    "\n",
    "print \"# paragraphs in translated: %d\"%len(tran_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "translated_pars = []\n",
    "translated_pars = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(pre_process_par), tran_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "translated_graphs = []\n",
    "translated_graphs = Parallel(n_jobs=-1, verbose=-1, backend=\"multiprocessing\")(\n",
    "             map(delayed(create_graph), translated_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trans_pars_graphs.pickle','wb') as out:\n",
    "    pickle.dump(translated_graphs, out)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(translated_graphs,'./trans_graph_set.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dump_text = \"Barak is the US president . Angela is the chancellor of Germany . Barak and Angela met each other in France .\"\n",
    "#dump_text = \"Mohsen likes briliant people . Ali tries hard . Mohsen is a successful mohsen person . Mohsen is smart . \"\n",
    "#dump_text = pre_process_par(dump_text)\n",
    "#print dump_text\n",
    "#adj_list = create_graph(dump_text)\n",
    "#print word_connections\n",
    "#g = adj_list\n",
    "#for k, v in enumerate(g):\n",
    "#    print \"%d -->  %s\"%(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('original_pars_graphs.pickle','rb') as out:\n",
    "#    graphs  = pickle.load(out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data first. We have two corpora: \n",
    "         - original\n",
    "         - translated\n",
    "Each dataset consists of some paragraphs. Paragraphs are seperated by an empty line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of paragraphs. We need to pre-process each paragraph and remove all stop words from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should process and clean all pragraphs in both corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a paragraph and build its graph. In this regard we define a function that takes a list of sentenecs (tokenized) as an input text and returns a graph (in ?????? format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_text = \"Barak is the US president . Angela is the chancellor of Germany . Barak and Angela met each other in France .\"\n",
    "dump_text = \"Mohsen likes Ali people . Mohsen tries Ali hard . Mohsen is a successful person . Mohsen is smart . \"\n",
    "dump_text = pre_process_par(dump_text)\n",
    "print dump_text\n",
    "adj_list = create_graph(dump_text)\n",
    "\n",
    "g = adj_list\n",
    "for k, v in enumerate(g):\n",
    "    print \"%d -->  %s\"%(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_connections, adj_list = graphs[0]\n",
    "#for k,v in enumerate(adj_list):\n",
    "#    if len(v)>1:\n",
    "#        print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = graphs[0][1]\n",
    "#for k, v in enumerate(g):\n",
    "#    print \"%d --> %d %s\"%(k,v[0],v[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct graphs for translationese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
