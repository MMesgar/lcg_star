{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                  author: Mohsen Mesgar\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import sys, copy, time, math, pickle\n",
    "import cPickle as cpickle\n",
    "import itertools\n",
    "import scipy.io\n",
    "import pynauty\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys, getopt\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from subgraph_struc import read, write,get_canonical_map,draw, graph_to_adj_matrix as graph2am, recover_graph, draw\n",
    "from graph_set import read_graph_set as read_gs\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def drawProgressBar(shell_out, \n",
    "                    begin, k, out_of, end, barLen =25):\n",
    "    percent = k/float(out_of)\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        elif i==int(barLen * percent):\n",
    "            progress +='>'\n",
    "        else:\n",
    "            progress += \"_\"\n",
    "    text = \"%s%d/%d[%s](%.2f%%)%s\"%(begin,k,out_of,progress,percent * 100, end)\n",
    "    if shell_out== True:\n",
    "        sys.stdout.write(text)\n",
    "        sys.stdout.flush()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                   get pattern canonical map without node order\n",
    "###############################################################################\n",
    "def get_canonical_map(g):\n",
    "    if len(g.nodes())>0:\n",
    "        a = nx.adjacency_matrix(g)\n",
    "        am = a.todense()\n",
    "        window = np.array(am)\n",
    "        adj_mat = {idx: [i for i in list(np.where(edge)[0]) if i!=idx] for idx, edge in enumerate(window)}\n",
    "#       This line doesn't take into account the order of nodes, it produces the identical\n",
    "#       canonoical map for these graphs\n",
    "#       0-->1 2, 0 1-->2, 0-->2 1\n",
    "#        tmp = pynauty.Graph(number_of_vertices=len(g.nodes()), directed=True, adjacency_dict = adj_mat) \n",
    "\n",
    "        tmp = pynauty.Graph(number_of_vertices=len(g.nodes()), directed=True, adjacency_dict = adj_mat, \n",
    "                    vertex_coloring = [set([t]) for t in range(len(g.nodes(0)))],) \n",
    "\n",
    "        cert = pynauty.certificate(tmp)\n",
    "    else:\n",
    "        cert = ''\n",
    "    return cert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                               read graph maps\n",
    "###############################################################################\n",
    "def get_maps(can_map_file, count_file):\n",
    "    # canonical_map -> {canonical string id: {\"graph\", \"idx\", \"n\"}}\n",
    "    canonical_map = read(can_map_file)\n",
    "    \n",
    "   \n",
    "   # weight map -> {parent id: {child1: weight1, ...}}\n",
    "    weight_map = read(count_file)\n",
    "    \n",
    "    \n",
    "    weight_map = {parent: {child: weight/float(sum(children.values())) for child, weight in children.items()} \n",
    "                    for parent, children in weight_map.items()}\n",
    "    child_map = {}\n",
    "    for parent, children in weight_map.items():\n",
    "        for k,v in children.items():\n",
    "            if k not in child_map:\n",
    "                child_map[k] = {}\n",
    "            child_map[k][parent] = v\n",
    "    weight_map = child_map\n",
    "    return canonical_map, weight_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                  compute the base probability\n",
    "###############################################################################\n",
    "def pb(graph_id, weight_map):\n",
    "    parents =  weight_map[graph_id] \n",
    "    total = 0    \n",
    "    for k,w in parents.items():\n",
    "        total = w*pb(k, weight_map)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "###############################################################################\n",
    "##                compute the count of each pattrn in each graph\n",
    "###############################################################################\n",
    "def pattern_counter_in_graph(inputs):\n",
    "    gidx = inputs[0]\n",
    "    graph = inputs[1]\n",
    "    min_pattern_size = inputs[2] \n",
    "    max_pattern_size = inputs[3]\n",
    "    samplesize = inputs[4] \n",
    "    canonical_map = inputs[5]\n",
    "    \n",
    "    # in case we don't observe any graphlet in the graph, we fallback to the graphlet id that has zero edges in it\n",
    "    #fallback_map = {1: 1, 2: 2, 3: 4, 4: 8, 5: 19, 6: 53,       7: 209, 8: 1253, 9: 13599}\n",
    "    fallback_map = {1: 1, 2: 3, 3: 11, 4: 75, 5: 1099, 6: 13901}\n",
    "    # initialize the seed \t\n",
    "    seed = 1        \n",
    "    np.random.seed(seed)   \n",
    "    \n",
    "    am = graph2am(graph)\n",
    "    graph_size = len(am)\n",
    "    \n",
    "    # count_map = {node id: absolute count, ...}\n",
    "    count_map = {}\n",
    "    \n",
    "    \n",
    "    for pattern_size in range(min_pattern_size, max_pattern_size+1):\n",
    "        #print \"pattern_size=\", pattern_size\n",
    "        \n",
    "        # we don't need to loop if size of the adj. matrix is smaller than n        \n",
    "        if graph_size >= pattern_size:\n",
    "            count = 0\n",
    "            sample_set =[]\n",
    "            ub = scipy.misc.comb(graph_size, pattern_size)\n",
    "            while (len(sample_set) <= samplesize and len(sample_set) < ub):\n",
    "                #print \"sample_set=\", sample_set\n",
    "                r = random.sample(range(graph_size), pattern_size)\n",
    "                r_sort = np.sort(r).tolist()\n",
    "                \n",
    "                #print \"r\",r\n",
    "                #print \"r_sort\",r_sort\n",
    "                \n",
    "                if sample_set.count(r_sort)==0:\n",
    "                    sample_set.append(r_sort)\n",
    "                    count = count + 1\n",
    "                #print \"count\",count\n",
    "                \n",
    "        #    for s in range(samplesize):\n",
    "            #print \"final_sample_set=\", sample_set\n",
    "            #print \"final_count\", count\n",
    "            for s in sample_set:\n",
    "                #print \"sample=\",s\n",
    "                window = am[np.ix_(s,s)]\n",
    " \n",
    "                # fekr konam window bayyad ye jori graph bashe\n",
    "                pattern = nx.DiGraph(window)\n",
    "                g_type = canonical_map[get_canonical_map(pattern)][\"idx\"]               \n",
    "                #print \"g_type\", g_type\n",
    "                \n",
    "                # increment the count of seen graphlet\n",
    "                count_map[g_type] = count_map.get(g_type,0)+ 1.0\n",
    "                #print  \"count_map[g_type]\", count_map[g_type]\n",
    "\n",
    "        else:\n",
    "            # fallback to 0th node at that level\n",
    "            count_map[fallback_map[pattern_size]] = samplesize\n",
    "            #print \"In_fall_back\",\"count_map[fallback_map[pattern_size]]\",count_map[fallback_map[pattern_size]]\n",
    "  \n",
    "    return (gidx, count_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##           compute the count of subgraphs for each graph in graph set\n",
    "## graph_set: a dictionary of graphs and their id. {idx1:graph1, idx2:graph2,...}\n",
    "###############################################################################\n",
    "from joblib import Parallel, delayed\n",
    "def count_subgraphs(graph_set_file, min_pattern_size, max_pattern_size, sample_size, can_map, output_file):\n",
    " \n",
    "    ## read graph_set\n",
    "    print \"loading the graph_set_file ...: %s\"%graph_set_file\n",
    "    graph_set = read_graph_set(graph_set_file)\n",
    "    print \"# graphs in graph_set: %d\"%len(graph_set)\n",
    "\n",
    "    #    for gidx, value in graph_set.items():\n",
    "#        graph = value['graph']\n",
    "        #print \"graph_name =\"+ value['name'] +\" id=\" + str(gidx)\n",
    "#        graph_map[gidx] = sample_worker(graph,min_pattern_size,max_pattern_size, sample_size, can_map)\n",
    "        #print 'graph_'+str(gidx)+' is processed'\n",
    "    \n",
    "    print \"start counting patterns ...\"\n",
    "    input_graphs = [(gidx, value['graph'],min_pattern_size,max_pattern_size, sample_size, can_map) for gidx, value in graph_set.items()]  \n",
    "    \n",
    "    graph_map = []\n",
    "    for i,graph in enumerate(input_graphs):\n",
    "        graph_map.append(pattern_counter_in_graph(graph))\n",
    "        drawProgressBar(shell_out=True, \n",
    "                    begin=\"\", \n",
    "                        k=i+1, out_of=len(input_graphs), \n",
    "                        end=\"\")\n",
    "#     graph_map = Parallel(n_jobs=2, verbose=1, backend=\"multiprocessing\")(\n",
    "#        map(delayed(pattern_counter_in_graph), input_graphs))\n",
    "\n",
    "    ## which patterns occures how often\n",
    "    graph_map = { x:y for (x,y) in graph_map}   \n",
    "    \n",
    "    write(graph_map, output_file)\n",
    "    print \"\\ngraph_map is saved here : %s\"%output_file\n",
    "    return graph_map\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "###############################################################################\n",
    "##                               read graph set\n",
    "###############################################################################\n",
    "def read_graph_set(graph_set_file):\n",
    "    return read_gs(graph_set_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                      find the ids of all patterns with ps nodes\n",
    "## ps: pattern size\n",
    "###############################################################################\n",
    "def k_node_graphs(can_map, ps):\n",
    "    #output = {v['idx']  for k,v in can_map.items() if v['n']==ps }\n",
    "    output = {v['idx']  for k,v in can_map.items() if v['n']== ps and \n",
    "              nx.is_weakly_connected(get_subgraph(v['idx'], can_map))}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##         compute the sum over all count of k-node subgraphs\n",
    "## can_map = {graph_canonical_map:{'graph':..., 'idx':,..., 'n':....}}\n",
    "## k: k-node subgraphs, it shows the depth of the tree himap\n",
    "###############################################################################\n",
    "def z(all_knode_patterns, pat_cnt):\n",
    "    filter_pattern_count = {k:v for k,v in pat_cnt.items() if (k in all_knode_patterns)}\n",
    "    return float(0.1+sum([v for v in filter_pattern_count.values()]))\n",
    "    #return float(sum([v for v in filter_pattern_count.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                 n_c: number of paterns with exatly count e\n",
    "###############################################################################\n",
    "def n(e, pat_cnt):\n",
    "    l= pat_cnt.values()\n",
    "    return float(l.count(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                       compute discount value d\n",
    "## n_c:number of patterns with exactly count c\n",
    "###############################################################################\n",
    "def disc(c, pat_cnt):\n",
    "    n1 = n(1, pat_cnt)\n",
    "    n2 = n(2, pat_cnt)\n",
    "    n3 = n(3, pat_cnt)\n",
    "    n4 = n(4, pat_cnt)\n",
    "    y = float(n1) / n1+2*n2\n",
    "    if c==0:\n",
    "        return 0\n",
    "    elif c==1:\n",
    "        return y\n",
    "    elif c==2:\n",
    "        return 2-3*y*(float(n3)/n2)\n",
    "    else:\n",
    "        return 2-3*y*(float(n4)/n3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##               compute probability based on the frequency\n",
    "###############################################################################\n",
    "def pf(pattern_idx,pattern_count, d, z_value):\n",
    "    if (pattern_idx in pattern_count.keys()):\n",
    "        count = pattern_count[pattern_idx]\n",
    "    else:\n",
    "        count = 0\n",
    "   # d = disc(count, pattern_count)\n",
    "    nominator = max(count-d,0)\n",
    "    \n",
    "    denominator = z_value\n",
    "    prob =  float(nominator)/float(denominator)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                 Normalization factor for base probability\n",
    "###############################################################################\n",
    "def norm_fact(all_knode_patterns, pattern_count, d):\n",
    "    filter_pattern_count = {k:v for k,v in pattern_count.items() if (k in all_knode_patterns)}\n",
    "    num_nn = len([v for v in filter_pattern_count.values() if v >= d])\n",
    "    b= sum([v for v in filter_pattern_count.values() if v < d])\n",
    "    return num_nn, b\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "###############################################################################\n",
    "##                               Mass value\n",
    "###############################################################################\n",
    "def mass(d, z_value, norm_fact, bounes):    \n",
    "    return (d/z_value)*norm_fact +(bounes/z_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                       compute base probability of a pattern\n",
    "## pb('')=pb(1)=1 because those occur in every possible graph\n",
    "###############################################################################\n",
    "def pb(wm, parent_kn,  pattern_id):\n",
    "    prob_base = 0\n",
    "    if pattern_id ==0 :\n",
    "        prob_base=1\n",
    "    else:\n",
    "        for parent_id, weight in wm[pattern_id].items():\n",
    "            prob_base = prob_base + pb(wm,parent_kn, parent_id)*weight\n",
    "            #prob_base += (parent_kn[parent_id]*weight)\n",
    "    return prob_base  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##           KN probability of the given pattern in the given graph\n",
    "## pattern count == pc[graph_id]\n",
    "## ps is pattern_size= number of nodes\n",
    "###############################################################################\n",
    "def pkn(can_map, pattern_count, w_map,parent_kn, pattern_idx, pattern_size, d, z_value, all_knode_patterns):\n",
    "   # all_knode_patterns = k_node_graphs(can_map, pattern_size)\n",
    "   # z_value = z(all_knode_patterns, pattern_count)\n",
    "    p1= pf(pattern_idx, pattern_count, d, z_value)\n",
    "    if (d==0):\n",
    "        pkn = p1 \n",
    "    else:\n",
    "        p2 = pb(w_map,parent_kn, pattern_idx)\n",
    "        mass_factor , bonus = norm_fact(all_knode_patterns, pattern_count, d)\n",
    "        mass_value = mass(d, z_value,mass_factor, bonus)\n",
    "        pkn = p1 + (mass_value*p2)\n",
    "        \n",
    "    parent_kn[pattern_idx] = pkn\n",
    "    return pkn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                          compute graph vector\n",
    "## pc : pattern count in each graph of graph_set\n",
    "###############################################################################\n",
    "def get_graph_vector(pc, can_map, wei_map,parent_kn, number_nodes, d):\n",
    "    graph_vectores = {}\n",
    "    all_knode_patterns = k_node_graphs(can_map, number_nodes)\n",
    "    #print all_knode_patterns\n",
    "    for graph_id, patt_cnt in pc.items():\n",
    "        tmp_vect = {}\n",
    "        #print pc[graph_id]\n",
    "        #print all_knode_patterns\n",
    "        z_value = z(all_knode_patterns, pc[graph_id])\n",
    "        #print \"graph_id=\"+str(graph_id) + \" z_value=\" + str(z_value)\n",
    "        for pid in k_node_graphs(can_map, number_nodes):\n",
    "            p_pkn = pkn(can_map, pc[graph_id], wei_map,parent_kn, pid, number_nodes, d,z_value,all_knode_patterns)\n",
    "            tmp_vect[pid]=p_pkn\n",
    "        graph_vectores[graph_id] = tmp_vect\n",
    "    return graph_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                      find a graph in the can_map\n",
    "###############################################################################\n",
    "def get_subgraph(gidx, can_map):\n",
    "    tmp = [t for t in can_map.values() if t['idx']==int(gidx)][0]\n",
    "    graph= tmp['graph']\n",
    "    n= tmp['n']\n",
    "    g = recover_graph(graph,n, gidx)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##                 data points for classification\n",
    "###############################################################################    \n",
    "def data_points(hs, m):\n",
    "    instances = []\n",
    "    count = 0\n",
    "    for i in range(1,len(hs)+1):\n",
    "        for j in range(i+1, len(hs)+1):\n",
    "            label = -1 #'B'\n",
    "            d = hs[i]-hs[j]\n",
    "            if (math.fabs(d)>0.5):\n",
    "                if d>0:\n",
    "                    label = +1#'A'\n",
    "                count = count + 1\n",
    "                inst =m[i-1,:].tolist()[0] + m[j-1,:].tolist()[0]+[label]\n",
    "                instances.append(inst) \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count_of_connected_patterns_of_a_graph(pc_graph, can_map):\n",
    "    output = []\n",
    "    for idx in pc_graph.keys():\n",
    "        g = get_subgraph(idx,can_map)\n",
    "        if nx.is_weakly_connected(g):\n",
    "            #print \"idx: %d\"%idx\n",
    "            #print \"nodes : %s\"%g.nodes()\n",
    "            #print \"edges : %s\"%g.edges()\n",
    "            #print \"count : %d\"%pc_graph[idx]\n",
    "            #print \"------\"\n",
    "            output.append((idx,pc_graph[idx]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class count_matrix(object):\n",
    "    def __init__(self, name, pattern_ids, graph_ids,count_matrix):\n",
    "        self.pattern_ids = pattern_ids\n",
    "        self.graph_ids = graph_ids\n",
    "        self.count_matrix = count_matrix\n",
    "        self.name = name\n",
    "    \n",
    "    def display_patterns(self, can_map):\n",
    "        for idx in self.pattern_ids:\n",
    "            g = get_subgraph(idx,can_map)\n",
    "            print \"idx: %d\"%idx\n",
    "            print \"nodes : %s\"%g.nodes()\n",
    "            print \"edges : %s\"%g.edges()\n",
    "            print \"------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the graph_set_file ...: ./orig_graph_set.g\n",
      "# graphs in graph_set: 5000\n",
      "loading the graph_set_file ...: ./trans_graph_set.g\n",
      "# graphs in graph_set: 5000\n"
     ]
    }
   ],
   "source": [
    "x_orig = []\n",
    "y_orig = []\n",
    "x_tran = []\n",
    "y_tran = []\n",
    "max_len = 0\n",
    "for gs_id,graph_set_file in enumerate([\"./orig_graph_set.g\",\"./trans_graph_set.g\"]):\n",
    "\n",
    "    ## read graph_set\n",
    "    print \"loading the graph_set_file ...: %s\"%graph_set_file\n",
    "    graph_set = read_graph_set(graph_set_file)\n",
    "    print \"# graphs in graph_set: %d\"%len(graph_set)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for gidx, value in graph_set.items():\n",
    "        graph = value['graph']\n",
    "        D = nx.to_numpy_matrix(graph)\n",
    "        max_len = np.max([max_len,D.shape[0]])\n",
    "        if gs_id ==0:\n",
    "            x_orig.append(D)\n",
    "            y_orig.append(1)\n",
    "        else:\n",
    "            x_tran.append(D)\n",
    "            y_tran.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_orig_padded = []\n",
    "x_tran_padded = []\n",
    "for id,x_list in enumerate([x_orig,x_tran]):\n",
    "    for x in x_list:\n",
    "        x_padded = np.zeros((max_len,max_len))\n",
    "        x_padded[:x.shape[0],:x.shape[1]] = x\n",
    "        if id ==0:\n",
    "            x_orig_padded.append(x_padded)\n",
    "        else:\n",
    "            x_tran_padded.append(x_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 703)\n"
     ]
    }
   ],
   "source": [
    "print x_orig_padded[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    def forward(self, x):\n",
    "        # x.shape = (bacth_size,1, max_len, max_len)\n",
    "        x = self.conv1(x) # (bacth_size, 1, max_len-kernel_size+1,max_len-kernel_size+1 )\n",
    "        x = F.relu(x)# (bacth_size, 1, max_len-kernel_size+1,max_len-kernel_size+1 )\n",
    "        x= self.pool(x)# (bacth_size, 1, max_len-kernel_size+1/pool_size,\n",
    "        #max_len-kernel_size+1/pool_size )\n",
    "        return x\n",
    "\n",
    "\n",
    "# net = Net()\n",
    "# batch_size = 2\n",
    "# x = torch.from_numpy(np.array(x_orig_padded[:batch_size]))\n",
    "# x = Variable(x).type(torch.FloatTensor)\n",
    "# print x.size()\n",
    "# x = x.view(batch_size,1,max_len,max_len)\n",
    "# print x.size()\n",
    "# out_cnn = net(x)\n",
    "# print out_cnn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear = nn.Linear(self.input_size,2)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# out_cnn = out_cnn.view(batch_size,-1)\n",
    "# cls = classifier(350*350)\n",
    "# out_classifier = cls(out_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x_orig + x_tran\n",
    "y = [1]*5000 + [0]*5000\n",
    "\n",
    "#shuffle the data\n",
    "tmp = list(zip(x, y))\n",
    "rng = np.random.RandomState(0)\n",
    "rng.shuffle(tmp)\n",
    "x, y = zip(*tmp)\n",
    "\n",
    "x= np.array(x)\n",
    "y= np.array(y)\n",
    "\n",
    "# do cross_validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=1, random_state=rng) \n",
    "# for train, test in splits:\n",
    "#      print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = [(train,test) for train,test in rkf.split(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(X,Y, batch_size,max_len, cnn_model, cls_model, optimizer):\n",
    "    \n",
    "    num_batches = len(X) / batch_size\n",
    "    X = X[:num_batches*batch_size]\n",
    "    Y = Y[:num_batches*batch_size]\n",
    "    epoch_loss = 0.0\n",
    " \n",
    "    for batch_index in range(num_batches):\n",
    "        optimizer.zero_grad() \n",
    "        x_batch = X[batch_index*batch_size: (batch_index+1)*batch_size]\n",
    "        y_batch = Y[batch_index*batch_size: (batch_index+1)*batch_size]\n",
    "        \n",
    "        \n",
    "        x_batch_padded = []\n",
    "        for x in x_batch:\n",
    "            x_padded = np.zeros((max_len,max_len))\n",
    "            x_padded[:x.shape[0],:x.shape[1]] = x \n",
    "            x_batch_padded.append(x_padded)\n",
    "                \n",
    "        x = torch.from_numpy(np.array(x_batch_padded))\n",
    "        x = Variable(x).type(torch.FloatTensor)\n",
    "        x = x.view(batch_size,1,max_len,max_len)\n",
    "        if cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        cnn_out = cnn_model(x)\n",
    "        \n",
    "        cnn_out = cnn_out.view(batch_size,-1)\n",
    "        \n",
    "        out_classifier = cls_model(cnn_out)\n",
    "           \n",
    "        # compute the loss function or criteria\n",
    "        y = torch.from_numpy(np.array(y_batch))\n",
    "        y = Variable(y).type(torch.LongTensor)\n",
    "        if cuda.is_available():\n",
    "            y= y.cuda()\n",
    "        loss = F.cross_entropy(out_classifier, y)\n",
    "        \n",
    "        epoch_loss +=loss.data\n",
    "        \n",
    "        # update parameters\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X,Y,cnn_model,cls_model, batch_size=2):\n",
    "    \n",
    "    num_batches = len(X) / batch_size\n",
    "    X = X[:num_batches*batch_size]\n",
    "    Y = Y[:num_batches*batch_size]\n",
    "    \n",
    "    corrects = 0.0\n",
    "    num_test_samples = 0.0\n",
    "    for batch_index in range(num_batches):\n",
    "        \n",
    "        x_batch = X[batch_index*batch_size: (batch_index+1)*batch_size]\n",
    "        y_batch = Y[batch_index*batch_size: (batch_index+1)*batch_size]\n",
    "        \n",
    "        \n",
    "        x_batch_padded = []\n",
    "        for x in x_batch:\n",
    "            x_padded = np.zeros((max_len,max_len))\n",
    "            x_padded[:x.shape[0],:x.shape[1]] = x \n",
    "            x_batch_padded.append(x_padded)\n",
    "                \n",
    "        x = torch.from_numpy(np.array(x_batch_padded))\n",
    "        x = Variable(x).type(torch.FloatTensor)\n",
    "        x = x.view(batch_size,1,max_len,max_len)\n",
    "        if cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        \n",
    "        cnn_out = cnn_model(x)\n",
    "        \n",
    "        cnn_out = cnn_out.view(batch_size,-1)\n",
    "        \n",
    "        \n",
    "        out_classifier = cls_model(cnn_out)\n",
    "           \n",
    "        # compute the loss function or criteria\n",
    "        y = torch.from_numpy(np.array(y_batch))\n",
    "        y = Variable(y).type(torch.FloatTensor)\n",
    "        \n",
    "        predicted_label,predicted_index =  torch.topk(out_classifier,1)\n",
    "        predicted_index = predicted_index.type(torch.FloatTensor).squeeze()\n",
    "            \n",
    "        correct_in_batch =  sum(torch.eq(predicted_index,y).data)\n",
    "        \n",
    "        corrects +=  correct_in_batch\n",
    "        \n",
    "        num_test_samples += batch_size \n",
    "\n",
    "    acc = corrects / num_test_samples\n",
    "    return acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.732673\n",
      "acc_test= 0.493000\n",
      "loss: 0.693210\n",
      "acc_test= 0.497000\n",
      "loss: 0.693201\n",
      "acc_test= 0.497000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/mesgarmn/anaconda2/lib/python2.7/inspect.py\", line 1051, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/home/mesgarmn/anaconda2/lib/python2.7/inspect.py\", line 1011, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/mesgarmn/anaconda2/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/home/mesgarmn/anaconda2/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1412\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             )\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "i= 0 \n",
    "torch.manual_seed(0)\n",
    "from torch import cuda\n",
    "\n",
    "for train, test in datasets:   \n",
    "    if i>0:\n",
    "        break\n",
    "    i +=1\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    \n",
    "    #limit x_train,y_train\n",
    "    x_train = x_train\n",
    "    y_train = y_train\n",
    "    \n",
    "    x_test = x_test\n",
    "    y_test = y_test\n",
    "    # define model\n",
    "    cnn = Net()\n",
    "    classifier = Classifier(350*350) # how should we compute this 350?\n",
    "    if cuda.is_available():\n",
    "        cnn = cnn.cuda()\n",
    "        classifier = classifier.cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(list(cnn.parameters())+list(classifier.parameters()), lr=0.01)\n",
    "\n",
    "    #train the models\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_model(X=x_train,Y=y_train, \n",
    "                                 batch_size=25, max_len=703,\n",
    "                                 cnn_model=cnn, cls_model=classifier,\n",
    "                                 optimizer = optimizer)\n",
    "        if epoch % 10 == 0:\n",
    "            print \"loss: %f\"%epoch_loss[0]\n",
    "            print \"acc_test= %f\"%evaluate(x_test,y_test,cnn,classifier,1)\n",
    "    print \"acc_train= %f\"%evaluate(x_train,y_train,cnn,classifier,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n",
      "torch.Size([1, 491401])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-635b0fcdb4e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"acc_test= %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-c3c21292c0fb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X, Y, cnn_model, cls_model, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/tensor.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, dest_type)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print \"acc_test= %f\"%evaluate(x_test,y_test,cnn,classifier,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##  Here we go step by step over our experiments on translationese and original\n",
    "###############################################################################\n",
    "def hansard_experiment(num_nodes):\n",
    "    min_pattern_size = num_nodes\n",
    "    max_pattern_size = num_nodes\n",
    "    sample_size = 2000 # numbrt of samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    normalized = True\n",
    "    print \"min_pattern_size: %d\"%min_pattern_size\n",
    "    print \"max_pattern_size: %d\"%max_pattern_size\n",
    "    print \"sample_size: %d\"%sample_size\n",
    "    \n",
    "    \n",
    "    can_map_file = \"./canonical_map/can_map_maxk6.p\"\n",
    "    himap_file = \"./canonical_map/himap_maxk6.p\"\n",
    "\n",
    "    \n",
    "    subgraph_count_file = \"./count_orig_graph_set\"+\"_min:\"+ str(min_pattern_size)+\"_max:\"+str(max_pattern_size)\n",
    "    \n",
    "    print \"loading can_map and hi_map: %s %s\"%(can_map_file, himap_file)\n",
    "    can_map, weight_map = get_maps(can_map_file, himap_file)    \n",
    "       \n",
    "    output = []\n",
    "    for gs_id,graph_set_file in enumerate([\"./orig_graph_set.g\",\"./trans_graph_set.g\"]):\n",
    "        print \"processing: %s \"%graph_set_file\n",
    "        \n",
    "        pc = count_subgraphs(graph_set_file,\n",
    "                             min_pattern_size, max_pattern_size,\n",
    "                            sample_size,\n",
    "                             can_map, \n",
    "                             subgraph_count_file)\n",
    "\n",
    "        print \"pattern counting is done.\"\n",
    "    \n",
    "        all_count_matrices = {}\n",
    "        print \"computing the count matrices ...\"\n",
    "        for num_nodes in range(min_pattern_size,max_pattern_size+1):\n",
    "            #print \"pattern_size: %d\"%num_nodes\n",
    "            connected_patterns_idx = list(k_node_graphs(can_map,num_nodes))\n",
    "            #print \"list of all possible connected patterns (columns): %s\"%connected_patterns_idx\n",
    "            num_graphs = len(pc.keys())\n",
    "            num_patterns = len(connected_patterns_idx)\n",
    "            cnt_matrix = np.zeros((num_graphs,num_patterns))\n",
    "            #print \"graph ids in rows of count_matrix: %s\" %pc.keys()\n",
    "            for key in pc.keys():\n",
    "                count  = get_count_of_connected_patterns_of_a_graph(pc[key], can_map)\n",
    "                row = key\n",
    "                for (pattern_id, value) in count:\n",
    "                    if pattern_id not in connected_patterns_idx:\n",
    "                        continue\n",
    "                    col = connected_patterns_idx.index(pattern_id)\n",
    "                    cnt_matrix[row, col] = value\n",
    "            cm = count_matrix(num_nodes, connected_patterns_idx,pc.keys(),  cnt_matrix)\n",
    "            all_count_matrices[num_nodes] = cm \n",
    "        print \"all connected patterns are counted\"\n",
    "        output.append((graph_set_file, pc, can_map, all_count_matrices))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "##  Here we go step by step over our experiments on translationese and original\n",
    "###############################################################################\n",
    "def hansard_baseline(num_nodes):\n",
    "    min_pattern_size = num_nodes\n",
    "    max_pattern_size = num_nodes\n",
    "    sample_size = 2000 # numbrt of samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    normalized = True\n",
    "    print \"min_pattern_size: %d\"%min_pattern_size\n",
    "    print \"max_pattern_size: %d\"%max_pattern_size\n",
    "    print \"sample_size: %d\"%sample_size\n",
    "    \n",
    "    \n",
    "    can_map_file = \"./canonical_map/can_map_maxk6.p\"\n",
    "    himap_file = \"./canonical_map/himap_maxk6.p\"\n",
    "\n",
    "    \n",
    "    subgraph_count_file = \"./count_orig_graph_set\"+\"_min:\"+ str(min_pattern_size)+\"_max:\"+str(max_pattern_size)\n",
    "    \n",
    "    print \"loading can_map and hi_map: %s %s\"%(can_map_file, himap_file)\n",
    "    can_map, weight_map = get_maps(can_map_file, himap_file)    \n",
    "       \n",
    "    output = []\n",
    "    for gs_id,graph_set_file in enumerate([\"./orig_lcg_graph_set.g\",\"./trans_lcg_graph_set.g\"]):\n",
    "        print \"processing: %s \"%graph_set_file\n",
    "        \n",
    "        pc = count_subgraphs(graph_set_file,\n",
    "                             min_pattern_size, max_pattern_size,\n",
    "                            sample_size,\n",
    "                             can_map, \n",
    "                             subgraph_count_file)\n",
    "\n",
    "        print \"pattern counting is done.\"\n",
    "    \n",
    "        all_count_matrices = {}\n",
    "        print \"computing the count matrices ...\"\n",
    "        for num_nodes in range(min_pattern_size,max_pattern_size+1):\n",
    "            #print \"pattern_size: %d\"%num_nodes\n",
    "            connected_patterns_idx = list(k_node_graphs(can_map,num_nodes))\n",
    "            #print \"list of all possible connected patterns (columns): %s\"%connected_patterns_idx\n",
    "            num_graphs = len(pc.keys())\n",
    "            num_patterns = len(connected_patterns_idx)\n",
    "            cnt_matrix = np.zeros((num_graphs,num_patterns))\n",
    "            #print \"graph ids in rows of count_matrix: %s\" %pc.keys()\n",
    "            for key in pc.keys():\n",
    "                count  = get_count_of_connected_patterns_of_a_graph(pc[key], can_map)\n",
    "                row = key\n",
    "                for (pattern_id, value) in count:\n",
    "                    if pattern_id not in connected_patterns_idx:\n",
    "                        continue\n",
    "                    col = connected_patterns_idx.index(pattern_id)\n",
    "                    cnt_matrix[row, col] = value\n",
    "            cm = count_matrix(num_nodes, connected_patterns_idx,pc.keys(),  cnt_matrix)\n",
    "            all_count_matrices[num_nodes] = cm \n",
    "        print \"all connected patterns are counted\"\n",
    "        output.append((graph_set_file, pc, can_map, all_count_matrices))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### LCG count 3-node\n",
    "output = hansard_baseline(num_nodes=3)\n",
    "# with open('./final_pattern_count_3_lcg.pkl','wb') as h:\n",
    "#     cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 3-node count\n",
    "with open('./final_pattern_count_3_lcg.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_nodes_original = all_count_matrices_original[3]\n",
    "#three_nodes_original.display_patterns(can_map_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_nodes_trans = all_count_matrices_trans[3]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_original = softmax(three_nodes_original.count_matrix)\n",
    "x_original = three_nodes_original.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_trans = softmax(three_nodes_trans.count_matrix)\n",
    "x_trans = three_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =  np.concatenate((x_original, x_trans), axis=0)\n",
    "y = [1]*5000 + [0]*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s =  x.sum(1,keepdims=True)*1.0 \n",
    "x = x/ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_tsne = TSNE(learning_rate=100).fit_transform(x)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_3node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_3node_LR+= list(predicted)\n",
    "print \"acc 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## comute 4-node count\n",
    "output = hansard_experiment(num_nodes=4)\n",
    "with open('./final_pattern_count_4_lcg.pkl','wb') as h:\n",
    "     cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 4node count\n",
    "with open('./final_pattern_count_4_lcg.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_nodes_original = all_count_matrices_original[4]\n",
    "#three_nodes_original.display_patterns(can_map_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_nodes_trans = all_count_matrices_trans[4]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_original = softmax(four_nodes_original.count_matrix)\n",
    "x_original = four_nodes_original.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_trans = softmax(four_nodes_trans.count_matrix)\n",
    "x_trans = four_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =  np.concatenate((x_original, x_trans), axis=0)\n",
    "y = [1]*5000 + [0]*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = x.sum(1,keepdims=True)*1.0\n",
    "x = x/ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_tsne = TSNE(learning_rate=100).fit_transform(x)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_4node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_4node_LR += list(predicted)\n",
    "print \"acc 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)\n",
    "\n",
    "s =  x.sum(1,keepdims=True)*1.0 \n",
    "x = x/ s\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_tsne = TSNE(learning_rate=100).fit_transform(x)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_3node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_3node_LR+= list(predicted)\n",
    "print \"acc 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "###############################################################################\n",
    "##                               Main\n",
    "###############################################################################\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "\n",
    "#output = hansard_experiment(num_nodes=3)\n",
    "#with open('./final_pattern_count_3.pkl','wb') as h:\n",
    "#    cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 3-node count\n",
    "with open('./final_pattern_count_3.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_nodes_original = all_count_matrices_original[3]\n",
    "#three_nodes_original.display_patterns(can_map_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_nodes_trans = all_count_matrices_trans[3]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x_original = softmax(three_nodes_original.count_matrix)\n",
    "x_original = three_nodes_original.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_trans = softmax(three_nodes_trans.count_matrix)\n",
    "x_trans = three_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =  np.concatenate((x_original, x_trans), axis=0)\n",
    "y = [1]*5000 + [0]*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s =  x.sum(1,keepdims=True)*1.0 \n",
    "x = x/ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_tsne = TSNE(learning_rate=100).fit_transform(x)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_3node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_3node_LR+= list(predicted)\n",
    "print \"acc 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random classification\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.dummy import DummyClassifier as baseline\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = baseline(strategy='most_frequent', random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_majority =[]\n",
    "\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    predictions_majority += list(predicted)\n",
    "    print i,acc\n",
    "print \"acc 5-Majority = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind as ttest\n",
    "import numpy as np\n",
    "def significant_test(predictions_1, predictions_2):\n",
    "    predictions_1 = np.array(predictions_1)\n",
    "    predictions_2 = np.array(predictions_2)\n",
    "    _,p_value = ttest(predictions_1,predictions_2)\n",
    "    if p_value < 0.01:\n",
    "        print \"significant with p_value < 0.01\"\n",
    "    elif p_value < 0.05:\n",
    "        print \"significant with p_value < 0.05\"\n",
    "    else:\n",
    "        print \"NOT significant\"\n",
    "    return p_value\n",
    "significant_test(predictions_majority,predictions_3node_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## comute 4-node count\n",
    "output = hansard_experiment(num_nodes=4)\n",
    "# with open('./final_pattern_count_4_4000.pkl','wb') as h:\n",
    "#     cpickle.dump(output,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 4node count\n",
    "with open('./final_pattern_count_4_4000.pkl','r') as h:\n",
    "    output = cpickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_original, can_map_original, all_count_matrices_original = \\\n",
    "    output[0][1],output[0][2],output[0][3]\n",
    "\n",
    "pc_trans, can_map_trans, all_count_matrices_trans = \\\n",
    "    output[1][1],output[1][2],output[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_nodes_original = all_count_matrices_original[4]\n",
    "#three_nodes_original.display_patterns(can_map_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_nodes_trans = all_count_matrices_trans[4]\n",
    "#three_nodes_trans.display_patterns(can_map_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_original = softmax(four_nodes_original.count_matrix)\n",
    "x_original = four_nodes_original.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_trans = softmax(four_nodes_trans.count_matrix)\n",
    "x_trans = four_nodes_trans.count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =  np.concatenate((x_original, x_trans), axis=0)\n",
    "y = [1]*5000 + [0]*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x,  y = shuffle(x,  y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = x.sum(1,keepdims=True)*1.0\n",
    "x = x/ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import figure\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_tsne = TSNE(learning_rate=100).fit_transform(x)\n",
    "\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import sklearn\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_4node_LR = []\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    print i,acc\n",
    "    predictions_4node_LR += list(predicted)\n",
    "print \"acc 5-CV = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random classification\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.dummy import DummyClassifier as baseline\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "\n",
    "classifier = baseline(strategy='most_frequent', random_state=random_state)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "mean_accs = []\n",
    "predictions_majority =[]\n",
    "\n",
    "for i in range(5):\n",
    "    predicted = cross_val_predict(classifier, x, y , cv=cv.split(x, y))\n",
    "    acc = accuracy_score(y, predicted)\n",
    "    mean_accs.append(acc)\n",
    "    predictions_majority += list(predicted)\n",
    "    print i,acc\n",
    "print \"acc 5-Majority = %.2f%%\"%(100*np.mean(mean_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "significant_test(predictions_3node_LR,predictions_4node_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
